#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„ç”Ÿäº§é”€å”®æ•°æ®åˆ†æç³»ç»Ÿ - æ•°æ®å¯¼å…¥å™¨
ç‰ˆæœ¬: 2.0
ä½œè€…: Kilo Code
æ—¥æœŸ: 2025-01-05

ä¸»è¦ä¼˜åŒ–:
1. å¢å¼ºçš„ETLæµç¨‹å’Œæ•°æ®éªŒè¯
2. æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
3. ä¼˜åŒ–çš„äº§é”€ç‡è®¡ç®—é€»è¾‘
4. å®æ—¶æ•°æ®åŒæ­¥æœºåˆ¶
5. æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Š
"""

import pandas as pd
import sqlite3
import numpy as np
import os
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import hashlib
import time

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DataQualityReport:
    """æ•°æ®è´¨é‡æŠ¥å‘Šæ•°æ®ç±»"""
    timestamp: str
    total_records: int
    valid_records: int
    invalid_records: int
    duplicate_records: int
    missing_values: Dict[str, int]
    data_type_errors: Dict[str, int]
    validation_errors: List[str]
    processing_time: float

@dataclass
class ETLConfig:
    """ETLé…ç½®æ•°æ®ç±»"""
    excel_folder: str = './Excelæ–‡ä»¶å¤¹/'
    db_path: str = 'backend/.wrangler/state/v3/d1/chunxue-prod-db.sqlite'
    sql_output_file: str = 'import_data.sql'
    backup_enabled: bool = True
    incremental_update: bool = True
    data_validation_enabled: bool = True
    unit_conversion_kg_to_tons: bool = True
    price_unit_conversion: bool = True

class DataValidator:
    """æ•°æ®éªŒè¯å™¨ç±»"""
    
    @staticmethod
    def validate_date(date_value) -> bool:
        """éªŒè¯æ—¥æœŸæ ¼å¼"""
        try:
            if pd.isna(date_value):
                return False
            pd.to_datetime(date_value)
            return True
        except:
            return False
    
    @staticmethod
    def validate_numeric(value, min_val: float = 0) -> bool:
        """éªŒè¯æ•°å€¼"""
        try:
            if pd.isna(value):
                return False
            num_val = float(value)
            return num_val >= min_val
        except:
            return False
    
    @staticmethod
    def validate_product_name(name) -> bool:
        """éªŒè¯äº§å“åç§°"""
        if pd.isna(name) or str(name).strip() == '':
            return False
        return True

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨ç±»"""
    
    def __init__(self, config: ETLConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.DataCleaner")
    
    def apply_business_filters(self, df: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """åº”ç”¨ä¸šåŠ¡è¿‡æ»¤è§„åˆ™"""
        original_count = len(df)
        self.logger.info(f"å¼€å§‹åº”ç”¨{data_type}ä¸šåŠ¡è¿‡æ»¤è§„åˆ™ï¼ŒåŸå§‹è®°å½•æ•°: {original_count}")
        
        if df.empty:
            return df
        
        # åˆ é™¤å…¨ç©ºè¡Œ
        df = df.dropna(how='all')
        
        # æ ¹æ®æ•°æ®ç±»å‹åº”ç”¨ç‰¹å®šè¿‡æ»¤è§„åˆ™
        if data_type == 'inventory':
            df = self._filter_inventory_data(df)
        elif data_type == 'production':
            df = self._filter_production_data(df)
        elif data_type == 'sales':
            df = self._filter_sales_data(df)
        
        filtered_count = len(df)
        self.logger.info(f"{data_type}æ•°æ®è¿‡æ»¤å®Œæˆ: {original_count} â†’ {filtered_count} æ¡è®°å½•")
        
        return df
    
    def _filter_inventory_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤åº“å­˜æ•°æ®"""
        # ä¿å­˜å‡¤è‚ äº§å“ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
        feng_chang_mask = df['ç‰©æ–™åç§°'].astype(str).str.contains('å‡¤è‚ ', case=False, na=False)
        feng_chang_products = df[feng_chang_mask].copy()
        
        # è¿‡æ»¤å®¢æˆ·åˆ—
        if 'å®¢æˆ·' in df.columns:
            excluded_customers = ['å‰¯äº§å“', 'é²œå“', '']
            df['å®¢æˆ·'] = df['å®¢æˆ·'].astype(str).str.strip()
            df = df[~df['å®¢æˆ·'].isin(excluded_customers)]
        
        # è¿‡æ»¤ç‰©æ–™åˆ†ç±»åç§°
        if 'ç‰©æ–™åˆ†ç±»åç§°' in df.columns:
            excluded_categories = ['å‰¯äº§å“', 'ç”Ÿé²œå“å…¶ä»–']
            df['ç‰©æ–™åˆ†ç±»åç§°'] = df['ç‰©æ–™åˆ†ç±»åç§°'].replace(r'^\s*$', np.nan, regex=True)
            mask = ~(df['ç‰©æ–™åˆ†ç±»åç§°'].isin(excluded_categories) | df['ç‰©æ–™åˆ†ç±»åç§°'].isna())
            df = df[mask]
        
        # æ’é™¤å«"é²œ"å­—çš„äº§å“
        df = df[~df['ç‰©æ–™åç§°'].astype(str).str.contains('é²œ', case=False, na=False)]
        
        # é‡æ–°æ·»åŠ å‡¤è‚ äº§å“
        if not feng_chang_products.empty:
            feng_chang_to_add = feng_chang_products[~feng_chang_products['ç‰©æ–™åç§°'].isin(df['ç‰©æ–™åç§°'])]
            if not feng_chang_to_add.empty:
                df = pd.concat([df, feng_chang_to_add], ignore_index=True)
                self.logger.info(f"é‡æ–°æ·»åŠ äº† {len(feng_chang_to_add)} æ¡å‡¤è‚ äº§å“è®°å½•")
        
        return df
    
    def _filter_production_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤ç”Ÿäº§æ•°æ®"""
        # æ’é™¤å«"é²œ"å­—çš„äº§å“
        if 'product_name' in df.columns:
            df = df[~df['product_name'].astype(str).str.contains('é²œ', case=False, na=False)]
        elif 'ç‰©æ–™åç§°' in df.columns:
            df = df[~df['ç‰©æ–™åç§°'].astype(str).str.contains('é²œ', case=False, na=False)]
        
        return df
    
    def _filter_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤é”€å”®æ•°æ®"""
        # è¿‡æ»¤ç‰©æ–™åˆ†ç±»
        if 'ç‰©æ–™åˆ†ç±»' in df.columns:
            df['ç‰©æ–™åˆ†ç±»'] = df['ç‰©æ–™åˆ†ç±»'].replace(r'^\s*$', np.nan, regex=True)
            df = df[~df['ç‰©æ–™åˆ†ç±»'].astype(str).str.lower().isin(['å‰¯äº§å“', 'nan', ''])]
        
        # è¿‡æ»¤å®¢æˆ·åç§°
        if 'å®¢æˆ·åç§°' in df.columns:
            df['å®¢æˆ·åç§°'] = df['å®¢æˆ·åç§°'].fillna('').astype(str).str.strip()
            excluded_customers = ['', 'å‰¯äº§å“', 'é²œå“']
            df = df[~df['å®¢æˆ·åç§°'].str.lower().isin([x.lower() for x in excluded_customers])]
        
        # æ’é™¤å«"é²œ"å­—çš„äº§å“
        if 'product_name' in df.columns:
            df = df[~df['product_name'].astype(str).str.contains('é²œ', case=False, na=False)]
        elif 'ç‰©æ–™åç§°' in df.columns:
            df = df[~df['ç‰©æ–™åç§°'].astype(str).str.contains('é²œ', case=False, na=False)]
        
        return df

class ProductionSalesRatioCalculator:
    """äº§é”€ç‡è®¡ç®—å™¨ç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.ProductionSalesRatioCalculator")
    
    def calculate_ratio(self, production_volume: float, sales_volume: float) -> float:
        """
        è®¡ç®—äº§é”€ç‡
        å…¬å¼: äº§é”€ç‡ = (é”€å”®é‡ / ç”Ÿäº§é‡) * 100%
        """
        try:
            if production_volume <= 0:
                return 0.0
            
            ratio = (sales_volume / production_volume) * 100
            
            # å¤„ç†å¼‚å¸¸å€¼
            if ratio > 1000:  # äº§é”€ç‡è¶…è¿‡1000%å¯èƒ½æ˜¯æ•°æ®é”™è¯¯
                self.logger.warning(f"å¼‚å¸¸äº§é”€ç‡æ£€æµ‹: {ratio:.2f}%, ç”Ÿäº§é‡: {production_volume}, é”€å”®é‡: {sales_volume}")
                return min(ratio, 1000)  # é™åˆ¶æœ€å¤§å€¼
            
            return round(ratio, 2)
        
        except (ZeroDivisionError, TypeError, ValueError) as e:
            self.logger.error(f"äº§é”€ç‡è®¡ç®—é”™è¯¯: {e}")
            return 0.0
    
    def calculate_inventory_turnover_days(self, inventory_level: float, avg_daily_sales: float) -> float:
        """
        è®¡ç®—åº“å­˜å‘¨è½¬å¤©æ•°
        å…¬å¼: åº“å­˜å‘¨è½¬å¤©æ•° = å½“å‰åº“å­˜ / æ—¥å‡é”€å”®é‡
        """
        try:
            if avg_daily_sales <= 0:
                return 0.0
            
            turnover_days = inventory_level / avg_daily_sales
            
            # å¤„ç†å¼‚å¸¸å€¼
            if turnover_days > 365:  # è¶…è¿‡ä¸€å¹´çš„å‘¨è½¬å¤©æ•°å¯èƒ½å¼‚å¸¸
                self.logger.warning(f"å¼‚å¸¸åº“å­˜å‘¨è½¬å¤©æ•°: {turnover_days:.2f}å¤©")
                return min(turnover_days, 365)
            
            return round(turnover_days, 2)
        
        except (ZeroDivisionError, TypeError, ValueError) as e:
            self.logger.error(f"åº“å­˜å‘¨è½¬å¤©æ•°è®¡ç®—é”™è¯¯: {e}")
            return 0.0

class OptimizedDataImporter:
    """ä¼˜åŒ–çš„æ•°æ®å¯¼å…¥å™¨ä¸»ç±»"""
    
    def __init__(self, config: ETLConfig = None):
        self.config = config or ETLConfig()
        self.logger = logging.getLogger(f"{__name__}.OptimizedDataImporter")
        self.data_cleaner = DataCleaner(self.config)
        self.ratio_calculator = ProductionSalesRatioCalculator()
        self.validator = DataValidator()
        
        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.quality_stats = {
            'total_processed': 0,
            'validation_errors': [],
            'data_type_errors': {},
            'missing_values': {}
        }
    
    def inspect_excel_structure(self) -> Dict[str, Any]:
        """æ£€æŸ¥Excelæ–‡ä»¶ç»“æ„"""
        self.logger.info("å¼€å§‹æ£€æŸ¥Excelæ–‡ä»¶ç»“æ„...")
        
        excel_files = {
            'production': os.path.join(self.config.excel_folder, 'äº§æˆå“å…¥åº“åˆ—è¡¨.xlsx'),
            'inventory': os.path.join(self.config.excel_folder, 'æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx'),
            'sales': os.path.join(self.config.excel_folder, 'é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx'),
            'price': os.path.join(self.config.excel_folder, 'ç»¼åˆå”®ä»·6.30.xlsx'),
            'adjustment': os.path.join(self.config.excel_folder, 'è°ƒä»·è¡¨.xlsx')
        }
        
        structure_info = {}
        
        for file_type, file_path in excel_files.items():
            if os.path.exists(file_path):
                try:
                    df = pd.read_excel(file_path, nrows=5)  # åªè¯»å–å‰5è¡Œç”¨äºç»“æ„æ£€æŸ¥
                    structure_info[file_type] = {
                        'path': file_path,
                        'columns': list(df.columns),
                        'shape': df.shape,
                        'exists': True
                    }
                    self.logger.info(f"{file_type} æ–‡ä»¶ç»“æ„: {df.shape}, åˆ—: {list(df.columns)}")
                except Exception as e:
                    structure_info[file_type] = {
                        'path': file_path,
                        'error': str(e),
                        'exists': True
                    }
                    self.logger.error(f"è¯»å– {file_type} æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            else:
                structure_info[file_type] = {
                    'path': file_path,
                    'exists': False
                }
                self.logger.warning(f"{file_type} æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        return structure_info
    
    def load_and_clean_excel(self, file_path: str, file_type: str) -> pd.DataFrame:
        """åŠ è½½å¹¶æ¸…æ´—Excelæ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹åŠ è½½ {file_type} æ•°æ®: {file_path}")
        
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path)
            self.logger.info(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # åº”ç”¨ä¸šåŠ¡è¿‡æ»¤è§„åˆ™
            df = self.data_cleaner.apply_business_filters(df, file_type)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œåˆ—æ˜ å°„å’Œæ•°æ®è½¬æ¢
            df = self._map_columns_and_convert(df, file_type)
            
            # æ•°æ®éªŒè¯
            if self.config.data_validation_enabled:
                df = self._validate_data(df, file_type)
            
            self.logger.info(f"{file_type} æ•°æ®åŠ è½½å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {df.shape}")
            return df
            
        except Exception as e:
            self.logger.error(f"åŠ è½½ {file_type} æ•°æ®æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    
    def _map_columns_and_convert(self, df: pd.DataFrame, file_type: str) -> pd.DataFrame:
        """åˆ—æ˜ å°„å’Œæ•°æ®è½¬æ¢"""
        if file_type == 'production':
            # ç”Ÿäº§æ•°æ®åˆ—æ˜ å°„
            column_mapping = {
                'å•æ®æ—¥æœŸ': 'record_date',
                'å…¥åº“æ—¥æœŸ': 'record_date', 
                'ç‰©æ–™åç§°': 'product_name',
                'ä¸»æ•°é‡': 'production_volume'
            }
            df = df.rename(columns=column_mapping)
            
            # å•ä½è½¬æ¢ï¼šå…¬æ–¤è½¬å¨
            if 'production_volume' in df.columns and self.config.unit_conversion_kg_to_tons:
                df['production_volume'] = pd.to_numeric(df['production_volume'], errors='coerce') / 1000
                self.logger.info("ç”Ÿäº§é‡å•ä½å·²ä»å…¬æ–¤è½¬æ¢ä¸ºå¨")
        
        elif file_type == 'inventory':
            # åº“å­˜æ•°æ®åˆ—æ˜ å°„
            column_mapping = {
                'ç‰©æ–™åç§°': 'product_name',
                'ç»“å­˜': 'inventory_level',
                'å…¥åº“': 'production_volume',
                'å‡ºåº“': 'sales_volume'
            }
            df = df.rename(columns=column_mapping)
            
            # å•ä½è½¬æ¢
            if self.config.unit_conversion_kg_to_tons:
                for col in ['inventory_level', 'production_volume', 'sales_volume']:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce') / 1000
                self.logger.info("åº“å­˜æ•°æ®å•ä½å·²ä»å…¬æ–¤è½¬æ¢ä¸ºå¨")
        
        elif file_type == 'sales':
            # é”€å”®æ•°æ®åˆ—æ˜ å°„
            column_mapping = {
                'å‘ç¥¨æ—¥æœŸ': 'record_date',
                'ç‰©æ–™åç§°': 'product_name',
                'ä¸»æ•°é‡': 'sales_volume'
            }
            df = df.rename(columns=column_mapping)
            
            # ä»·æ ¼å¤„ç†
            self._process_sales_price(df)
            
            # å•ä½è½¬æ¢
            if 'sales_volume' in df.columns and self.config.unit_conversion_kg_to_tons:
                df['sales_volume'] = pd.to_numeric(df['sales_volume'], errors='coerce') / 1000
                self.logger.info("é”€å”®é‡å•ä½å·²ä»å…¬æ–¤è½¬æ¢ä¸ºå¨")
        
        # ç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
        if 'record_date' in df.columns:
            df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce')
            df = df.dropna(subset=['record_date'])
            df['record_date'] = df['record_date'].dt.strftime('%Y-%m-%d')
        
        return df
    
    def _process_sales_price(self, df: pd.DataFrame):
        """å¤„ç†é”€å”®ä»·æ ¼æ•°æ®"""
        price_columns = [col for col in df.columns if 'å•ä»·' in col or 'ä»·æ ¼' in col]
        
        if 'æœ¬å¸å«ç¨å•ä»·' in df.columns:
            df['average_price'] = pd.to_numeric(df['æœ¬å¸å«ç¨å•ä»·'], errors='coerce')
            if self.config.price_unit_conversion:
                df['average_price'] = df['average_price'] * 1000  # å…ƒ/å…¬æ–¤ è½¬ å…ƒ/å¨
        elif 'å«ç¨å•ä»·' in df.columns:
            df['average_price'] = pd.to_numeric(df['å«ç¨å•ä»·'], errors='coerce')
            if self.config.price_unit_conversion:
                df['average_price'] = df['average_price'] * 1000
        elif 'æœ¬å¸æ— ç¨å•ä»·' in df.columns and 'æœ¬å¸æ— ç¨é‡‘é¢' in df.columns:
            # è®¡ç®—å«ç¨ä»·æ ¼
            df['æœ¬å¸æ— ç¨å•ä»·'] = pd.to_numeric(df['æœ¬å¸æ— ç¨å•ä»·'], errors='coerce')
            df['average_price'] = df['æœ¬å¸æ— ç¨å•ä»·'] * 1.09  # åŠ 9%ç¨ç‡
            if self.config.price_unit_conversion:
                df['average_price'] = df['average_price'] * 1000
        else:
            df['average_price'] = 0
            self.logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„ä»·æ ¼åˆ—ï¼Œä»·æ ¼è®¾ä¸º0")
    
    def _validate_data(self, df: pd.DataFrame, file_type: str) -> pd.DataFrame:
        """æ•°æ®éªŒè¯"""
        original_count = len(df)
        validation_errors = []
        
        # éªŒè¯äº§å“åç§°
        if 'product_name' in df.columns:
            invalid_names = ~df['product_name'].apply(self.validator.validate_product_name)
            if invalid_names.any():
                error_count = invalid_names.sum()
                validation_errors.append(f"{file_type}: {error_count} æ¡è®°å½•äº§å“åç§°æ— æ•ˆ")
                df = df[~invalid_names]
        
        # éªŒè¯æ—¥æœŸ
        if 'record_date' in df.columns:
            invalid_dates = ~df['record_date'].apply(self.validator.validate_date)
            if invalid_dates.any():
                error_count = invalid_dates.sum()
                validation_errors.append(f"{file_type}: {error_count} æ¡è®°å½•æ—¥æœŸæ— æ•ˆ")
        
        # éªŒè¯æ•°å€¼åˆ—
        numeric_columns = ['production_volume', 'sales_volume', 'inventory_level', 'average_price']
        for col in numeric_columns:
            if col in df.columns:
                invalid_values = ~df[col].apply(lambda x: self.validator.validate_numeric(x, -1000))  # å…è®¸è´Ÿå€¼ä½†æœ‰ä¸‹é™
                if invalid_values.any():
                    error_count = invalid_values.sum()
                    validation_errors.append(f"{file_type}: {error_count} æ¡è®°å½• {col} æ•°å€¼æ— æ•ˆ")
                    df.loc[invalid_values, col] = 0  # å°†æ— æ•ˆå€¼è®¾ä¸º0
        
        validated_count = len(df)
        if validation_errors:
            self.quality_stats['validation_errors'].extend(validation_errors)
            self.logger.warning(f"{file_type} æ•°æ®éªŒè¯: {original_count} â†’ {validated_count} æ¡è®°å½•")
        
        return df
    
    def calculate_dynamic_inventory(self, production_df: pd.DataFrame, sales_df: pd.DataFrame, 
                                  initial_inventory: Dict[str, float]) -> pd.DataFrame:
        """è®¡ç®—åŠ¨æ€åº“å­˜"""
        self.logger.info("å¼€å§‹è®¡ç®—åŠ¨æ€åº“å­˜...")
        
        # åˆå¹¶ç”Ÿäº§å’Œé”€å”®æ•°æ®
        all_dates = set()
        all_products = set()
        
        if not production_df.empty:
            all_dates.update(production_df['record_date'].unique())
            all_products.update(production_df['product_name'].unique())
        
        if not sales_df.empty:
            all_dates.update(sales_df['record_date'].unique())
            all_products.update(sales_df['product_name'].unique())
        
        # æŒ‰æ—¥æœŸæ’åº
        sorted_dates = sorted(all_dates)
        
        inventory_records = []
        
        for product in all_products:
            current_inventory = initial_inventory.get(product, 0)
            
            for date in sorted_dates:
                # è·å–å½“æ—¥ç”Ÿäº§é‡
                prod_mask = (production_df['record_date'] == date) & (production_df['product_name'] == product)
                daily_production = production_df[prod_mask]['production_volume'].sum() if not production_df.empty else 0
                
                # è·å–å½“æ—¥é”€å”®é‡
                sales_mask = (sales_df['record_date'] == date) & (sales_df['product_name'] == product)
                daily_sales = sales_df[sales_mask]['sales_volume'].sum() if not sales_df.empty else 0
                
                # è®¡ç®—å½“æ—¥åº“å­˜
                current_inventory = current_inventory + daily_production - daily_sales
                
                # è®°å½•åº“å­˜æ•°æ®
                inventory_records.append({
                    'record_date': date,
                    'product_name': product,
                    'inventory_level': current_inventory,
                    'daily_production': daily_production,
                    'daily_sales': daily_sales
                })
        
        inventory_df = pd.DataFrame(inventory_records)
        self.logger.info(f"åŠ¨æ€åº“å­˜è®¡ç®—å®Œæˆï¼Œç”Ÿæˆ {len(inventory_df)} æ¡è®°å½•")
        
        return inventory_df
    
    def generate_data_quality_report(self) -> DataQualityReport:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        return DataQualityReport(
            timestamp=datetime.now().isoformat(),
            total_records=self.quality_stats['total_processed'],
            valid_records=self.quality_stats['total_processed'] - len(self.quality_stats['validation_errors']),
            invalid_records=len(self.quality_stats['validation_errors']),
            duplicate_records=0,  # å¯ä»¥æ·»åŠ é‡å¤æ£€æµ‹é€»è¾‘
            missing_values=self.quality_stats['missing_values'],
            data_type_errors=self.quality_stats['data_type_errors'],
            validation_errors=self.quality_stats['validation_errors'],
            processing_time=0.0  # åœ¨ä¸»æµç¨‹ä¸­è®¡ç®—
        )
    
    def export_to_sql(self, products_df: pd.DataFrame, metrics_df: pd.DataFrame) -> str:
        """å¯¼å‡ºæ•°æ®åˆ°SQLæ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹å¯¼å‡ºSQLæ–‡ä»¶: {self.config.sql_output_file}")
        
        try:
            with open(self.config.sql_output_file, 'w', encoding='utf-8') as f:
                # å†™å…¥schema
                schema_path = 'backend/schema.sql'
                if os.path.exists(schema_path):
                    with open(schema_path, 'r', encoding='utf-8') as schema_f:
                        f.write(schema_f.read())
                        f.write('\n\n')
                
                # å†™å…¥äº§å“æ•°æ®
                f.write("-- äº§å“æ•°æ®æ’å…¥\n")
                for _, row in products_df.iterrows():
                    product_name_escaped = row['product_name'].replace("'", "''")
                    sql = f"INSERT INTO Products (product_id, product_name, sku, category) VALUES ({row['product_id']}, '{product_name_escaped}', NULL, NULL);\n"
                    f.write(sql)
                
                f.write("\n-- æ¯æ—¥æŒ‡æ ‡æ•°æ®æ’å…¥\n")
                # å†™å…¥æŒ‡æ ‡æ•°æ®
                for _, row in metrics_df.iterrows():
                    sql = f"INSERT INTO DailyMetrics (record_date, product_id, production_volume, sales_volume, inventory_level, average_price, inventory_turnover_days) VALUES ('{row['record_date']}', {row['product_id']}, {row.get('production_volume', 0)}, {row.get('sales_volume', 0)}, {row.get('inventory_level', 0)}, {row.get('average_price', 0)}, {row.get('inventory_turnover_days', 0)});\n"
                    f.write(sql)
            
            self.logger.info(f"SQLæ–‡ä»¶å¯¼å‡ºå®Œæˆ: {self.config.sql_output_file}")
            return self.config.sql_output_file
            
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºSQLæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    def run_etl_process(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ETLæµç¨‹"""
        start_time = time.time()
        self.logger.info("å¼€å§‹æ‰§è¡Œä¼˜åŒ–çš„ETLæµç¨‹...")
        
        try:
            # 1. æ£€æŸ¥æ–‡ä»¶ç»“æ„
            structure_info = self.inspect_excel_structure()
            
            # 2. åŠ è½½å’Œæ¸…æ´—æ•°æ®
            production_df = self.load_and_clean_excel(
                os.path.join(self.config.excel_folder, 'äº§æˆå“å…¥åº“åˆ—è¡¨.xlsx'), 'production'
            )
            
            inventory_df = self.load_and_clean_excel(
                os.path.join(self.config.excel_folder, 'æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx'), 'inventory'
            )
            
            sales_df = self.load_and_clean_excel(
                os.path.join(self.config.excel_folder, 'é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx'), 'sales'
            )
            
            # 3. åˆ›å»ºäº§å“ä¸»è¡¨
            all_products = set()
            for df in [production_df, inventory_df, sales_df]:
                if not df.empty and 'product_name' in df.columns:
                    all_products.update(df['product_name'].dropna().unique())
            
            products_df = pd.DataFrame(list(all_products), columns=['product_name'])
            products_df['product_id'] = range(1, len(products_df) + 1)
            product_mapping = products_df.set_index('product_name')['product_id'].to_dict()
            
            # 4. å¤„ç†æ¯æ—¥æŒ‡æ ‡æ•°æ®
            # èšåˆç”Ÿäº§æ•°æ®
            if not production_df.empty:
                production_daily = production_df.groupby(['record_date', 'product_name'])['production_volume'].sum().reset_index()
            else:
                production_daily = pd.DataFrame(columns=['record_date', 'product_name', 'production_volume'])
            
            # èšåˆé”€å”®æ•°æ®
            if not sales_df.empty:
                sales_df['total_amount'] = sales_df['sales_volume'] * sales_df['average_price']
                sales_daily = sales_df.groupby(['record_date', 'product_name']).agg({
                    'sales_volume': 'sum',
                    'total_amount': 'sum'
                }).reset_index()
                sales_daily['average_price'] = sales_daily['total_amount'] / sales_daily['sales_volume']
                sales_daily = sales_daily.drop(columns=['total_amount'])
            else:
                sales_daily = pd.DataFrame(columns=['record_date', 'product_name', 'sales_volume', 'average_price'])
            
            # åˆå¹¶æ•°æ®
            metrics_df = pd.merge(production_daily, sales_daily, on=['record_date', 'product_name'], how='outer')
            metrics_df = metrics_df.fillna(0)
            
            # æ·»åŠ äº§å“ID
            metrics_df['product_id'] = metrics_df['product_name'].map(product_mapping)
            
            # 5. è®¡ç®—åŠ¨æ€åº“å­˜
            if not inventory_df.empty:
                initial_inventory = inventory_df.set_index('product_name')['inventory_level'].to_dict()
            else:
                initial_inventory = {}
            
            inventory_calc_df = self.calculate_dynamic_inventory(production_df, sales_df, initial_inventory)
            
            # åˆå¹¶åº“å­˜æ•°æ®
            if not inventory_calc_df.empty:
                inventory_merge = inventory_calc_df[['record_date', 'product_name', 'inventory_level']]
                metrics_df = pd.merge(metrics_df, inventory_merge, on=['record_date', 'product_name'], how='left')
            
            # 6. è®¡ç®—äº§é”€ç‡å’Œåº“å­˜å‘¨è½¬å¤©æ•°
            metrics_df['production_sales_ratio'] = metrics_df.apply(
                lambda row: self.ratio_calculator.calculate_ratio(row['production_volume'], row['sales_volume']),
                axis=1
            )
            
            # è®¡ç®—åº“å­˜å‘¨è½¬å¤©æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å½“æ—¥é”€é‡ï¼‰
            metrics_df['inventory_turnover_days'] = metrics_df.apply(
                lambda row: self.ratio_calculator.calculate_inventory_turnover_days(
                    row.get('inventory_level', 0), row.get('sales_volume', 0)
                ), axis=1
            )
            
            # 7. å¯¼å‡ºSQLæ–‡ä»¶
            sql_file = self.export_to_sql(products_df, metrics_df)
            
            # 8. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
            processing_time = time.time() - start_time
            quality_report = self.generate_data_quality_report()
            quality_report.processing_time = processing_time
            
            # 9. è¿”å›å¤„ç†ç»“æœ
            result = {
                'success': True,
                'structure_info': structure_info,
                'products_count': len(products_df),
                'metrics_count': len(metrics_df),
                'sql_file': sql_file,
                'quality_report': quality_report,
                'processing_time': processing_time
            }
            
            self.logger.info(f"ETLæµç¨‹æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return result
            
        except Exception as e:
            self.logger.error(f"ETLæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ä¼˜åŒ–çš„ç”Ÿäº§é”€å”®æ•°æ®åˆ†æç³»ç»Ÿ - æ•°æ®å¯¼å…¥å™¨ v2.0")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = ETLConfig()
    
    # åˆ›å»ºå¯¼å…¥å™¨å®ä¾‹
    importer = OptimizedDataImporter(config)
    
    # è¿è¡ŒETLæµç¨‹
    result = importer.run_etl_process()
    
    if result['success']:
        print(f"\nâœ… ETLæµç¨‹æ‰§è¡ŒæˆåŠŸ!")
        print(f"ğŸ“Š å¤„ç†äº§å“æ•°é‡: {result['products_count']}")
        print(f"ğŸ“ˆ ç”ŸæˆæŒ‡æ ‡è®°å½•: {result['metrics_count']}")
        print(f"ğŸ“„ SQLæ–‡ä»¶: {result['sql_file']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
        
        # æ‰“å°æ•°æ®è´¨é‡æŠ¥å‘Š
        quality_report = result['quality_report']
        print(f"\nğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"   æ€»è®°å½•æ•°: {quality_report.total_records}")
        print(f"   æœ‰æ•ˆè®°å½•: {quality_report.valid_records}")
        print(f"   æ— æ•ˆè®°å½•: {quality_report.invalid_records}")
        if quality_report.validation_errors:
            print(f"   éªŒè¯é”™è¯¯: {len(quality_report.validation_errors)}")
            for error in quality_report.validation_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"     - {error}")
    else:
        print(f"\nâŒ ETLæµç¨‹æ‰§è¡Œå¤±è´¥: {result['error']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()