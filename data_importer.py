import pandas as pd
import sqlite3
import numpy as np
from sqlalchemy import create_engine
import os

def inspect_excel_files():
    """
    æ£€æŸ¥Excelæ–‡ä»¶çš„åˆ—ç»“æ„ï¼Œå¸®åŠ©è¯Šæ–­æ•°æ®å¯¼å…¥é—®é¢˜
    """
    excel_files = {
        'inbound': 'Excelæ–‡ä»¶å¤¹/äº§æˆå“å…¥åº“åˆ—è¡¨.xlsx',
        'summary': 'Excelæ–‡ä»¶å¤¹/æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx',
        'sales': 'Excelæ–‡ä»¶å¤¹/é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx',
    }

    for file_type, file_path in excel_files.items():
        if os.path.exists(file_path):
            print(f"\n=== {file_type.upper()} FILE: {file_path} ===")
            try:
                df = pd.read_excel(file_path, sheet_name=0)
                print(f"Shape: {df.shape}")
                print(f"Columns: {list(df.columns)}")
                print(f"First few rows:")
                print(df.head(3))
                print(f"Data types:")
                print(df.dtypes)
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
        else:
            print(f"File not found: {file_path}")
    print("\n" + "="*80)

# --- é…ç½® ---
# è¯·æ›¿æ¢ä¸ºæ‚¨çš„Excelæ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿è¿™äº›æ–‡ä»¶åœ¨ 'Excelæ–‡ä»¶å¤¹' å†…
EXCEL_FILES = {
    'inbound': 'Excelæ–‡ä»¶å¤¹/äº§æˆå“å…¥åº“åˆ—è¡¨.xlsx',
    'summary': 'Excelæ–‡ä»¶å¤¹/æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx',
    'sales': 'Excelæ–‡ä»¶å¤¹/é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx',
    # å¦‚æœæœ‰å…¶ä»–Excelæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
}
DB_NAME = 'temp_db.sqlite'                 # ä¸´æ—¶SQLiteæ•°æ®åº“æ–‡ä»¶å
SQL_OUTPUT_FILE = 'import_data.sql'        # æœ€ç»ˆç”Ÿæˆçš„SQLæ–‡ä»¶å
DB_TABLE_PRODUCTS = 'Products'
DB_TABLE_METRICS = 'DailyMetrics'

def apply_inventory_data_filters(inventory_df):
    """
    åº”ç”¨åº“å­˜æ•°æ®è¿‡æ»¤é€»è¾‘ï¼Œå‚è€ƒåŸå§‹Pythonè„šæœ¬ä¸­çš„data_loader.py
    """
    if inventory_df.empty:
        return inventory_df

    print(f"  ğŸ“Š Applying inventory data filters (original shape: {inventory_df.shape})")

    # åˆ é™¤å…¨ç©ºè¡Œå’Œå“åä¸ºç©ºçš„è¡Œ
    inventory_df = inventory_df.dropna(how='all')
    inventory_df = inventory_df[inventory_df['ç‰©æ–™åç§°'].notna() & (inventory_df['ç‰©æ–™åç§°'] != '')]

    # åœ¨è¿‡æ»¤å‰ä¿å­˜æ‰€æœ‰"å‡¤è‚ "äº§å“è®°å½•ï¼Œä»¥ä¾¿åç»­æ·»åŠ å›æ¥
    feng_chang_products = inventory_df[inventory_df['ç‰©æ–™åç§°'].astype(str).str.contains('å‡¤è‚ ', case=False, na=False)].copy()
    if not feng_chang_products.empty:
        print(f"  ğŸ” Found {len(feng_chang_products)} å‡¤è‚  products, will preserve after filtering")

    # è¿‡æ»¤æ‰å®¢æˆ·ä¸º"å‰¯äº§å“"ã€"é²œå“"æˆ–ç©ºç™½çš„è®°å½•
    if 'å®¢æˆ·' in inventory_df.columns:
        original_count = len(inventory_df)
        inventory_df['å®¢æˆ·'] = inventory_df['å®¢æˆ·'].astype(str).str.strip()
        excluded_customers = ['å‰¯äº§å“', 'é²œå“', '']
        mask = ~inventory_df['å®¢æˆ·'].isin(excluded_customers)
        inventory_df = inventory_df[mask]
        print(f"  ğŸš« Excluded customers (å‰¯äº§å“, é²œå“, empty): {original_count} â†’ {len(inventory_df)} records")

    # æ ¹æ®"ç‰©æ–™åˆ†ç±»åç§°"åˆ—è¿›è¡Œç­›é€‰
    material_category_col = 'ç‰©æ–™åˆ†ç±»åç§°'
    if material_category_col in inventory_df.columns:
        excluded_categories = ['å‰¯äº§å“', 'ç”Ÿé²œå“å…¶ä»–']
        inventory_df[material_category_col] = inventory_df[material_category_col].replace(r'^\s*$', np.nan, regex=True)

        mask = ~(
            inventory_df[material_category_col].isin(excluded_categories) |
            inventory_df[material_category_col].isna()
        )
        original_count = len(inventory_df)
        inventory_df = inventory_df[mask]
        print(f"  ğŸš« Excluded material categories (å‰¯äº§å“, ç”Ÿé²œå“å…¶ä»–, empty): {original_count} â†’ {len(inventory_df)} records")

    # æ’é™¤ç‰©æ–™åç§°å«"é²œ"å­—çš„è®°å½•
    original_count = len(inventory_df)
    inventory_df = inventory_df[~inventory_df['ç‰©æ–™åç§°'].astype(str).str.contains('é²œ', case=False, na=False)]
    print(f"  ğŸš« Excluded products containing 'é²œ': {original_count} â†’ {len(inventory_df)} records")

    # ç‰¹æ®Šå¤„ç†ï¼šå°†"å‡¤è‚ "äº§å“æ·»åŠ å›æ¥
    if not feng_chang_products.empty:
        feng_chang_to_add = feng_chang_products[~feng_chang_products['ç‰©æ–™åç§°'].isin(inventory_df['ç‰©æ–™åç§°'])]
        if not feng_chang_to_add.empty:
            inventory_df = pd.concat([inventory_df, feng_chang_to_add], ignore_index=True)
            print(f"  âœ… Re-added {len(feng_chang_to_add)} å‡¤è‚  products back to inventory data")

    print(f"  âœ… Inventory filtering complete: final shape {inventory_df.shape}")
    return inventory_df

def apply_production_data_filters(production_df):
    """
    åº”ç”¨ç”Ÿäº§æ•°æ®è¿‡æ»¤é€»è¾‘ï¼Œå‚è€ƒåŸå§‹Pythonè„šæœ¬ä¸­çš„data_loader.py
    """
    if production_df.empty:
        return production_df

    print(f"  ğŸ“Š Applying production data filters (original shape: {production_df.shape})")

    # åˆ é™¤å…¨ç©ºè¡Œ
    production_df = production_df.dropna(how='all')

    # æ•°æ®æ¸…æ´—ï¼šå»é™¤ç‰©æ–™åç§°ä¸­å«"é²œ"å­—çš„è¡Œ
    if 'product_name' in production_df.columns:
        original_count = len(production_df)
        production_df = production_df[~production_df['product_name'].astype(str).str.contains('é²œ', case=False, na=False)]
        print(f"  ğŸš« Excluded products containing 'é²œ': {original_count} â†’ {len(production_df)} records")

    print(f"  âœ… Production filtering complete: final shape {production_df.shape}")
    return production_df

def apply_sales_data_filters(sales_df):
    """
    åº”ç”¨é”€å”®æ•°æ®è¿‡æ»¤é€»è¾‘ï¼Œå‚è€ƒåŸå§‹Pythonè„šæœ¬ä¸­çš„data_loader.py
    """
    if sales_df.empty:
        return sales_df

    print(f"  ğŸ“Š Applying sales data filters (original shape: {sales_df.shape})")

    # æ¸…æ´—æ¡ä»¶ 1: ç‰©æ–™åˆ†ç±»åˆ—ï¼Œå»é™¤"å‰¯äº§å“"ã€"ç©ºç™½"çš„è¡Œ
    material_category_column = 'ç‰©æ–™åˆ†ç±»'
    if material_category_column in sales_df.columns:
        original_count = len(sales_df)
        sales_df[material_category_column] = sales_df[material_category_column].replace(r'^\s*$', np.nan, regex=True)
        sales_df = sales_df[
            (~sales_df[material_category_column].astype(str).str.lower().isin(['å‰¯äº§å“', 'nan', '']))
        ]
        print(f"  ğŸš« Excluded material categories (å‰¯äº§å“, empty): {original_count} â†’ {len(sales_df)} records")

    # æ¸…æ´—æ¡ä»¶ 2: å®¢æˆ·åç§°åˆ—ï¼Œæ’é™¤å®¢æˆ·åç§°ä¸ºç©ºã€"å‰¯äº§å“"æˆ–"é²œå“"çš„è®°å½•
    customer_name_column = 'å®¢æˆ·åç§°'
    if customer_name_column in sales_df.columns:
        original_count = len(sales_df)
        sales_df[customer_name_column] = sales_df[customer_name_column].fillna('').astype(str).str.strip()
        excluded_customer_names_lower = ['', 'å‰¯äº§å“'.lower(), 'é²œå“'.lower()]
        sales_df = sales_df[~sales_df[customer_name_column].str.lower().isin(excluded_customer_names_lower)]
        print(f"  ğŸš« Excluded customers (empty, å‰¯äº§å“, é²œå“): {original_count} â†’ {len(sales_df)} records")

    # æ¸…æ´—æ¡ä»¶ 3: ç‰©æ–™åç§°åˆ—ï¼Œåˆ é™¤å…¶ä¸­åŒ…å«"é²œ"çš„è®°å½•
    if 'product_name' in sales_df.columns:
        original_count = len(sales_df)
        sales_df = sales_df[
            ~sales_df['product_name'].astype(str).str.contains('é²œ', case=False, na=False)
        ]
        print(f"  ğŸš« Excluded products containing 'é²œ': {original_count} â†’ {len(sales_df)} records")

    print(f"  âœ… Sales filtering complete: final shape {sales_df.shape}")
    return sales_df

def clean_and_load_excel(file_path, file_type, sheet_name=0):
    """
    è¯»å–Excelæ–‡ä»¶å¹¶è¿›è¡Œåˆæ­¥æ¸…æ´—ã€‚
    æ­¤å‡½æ•°æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒæ•´åˆ—åï¼Œå¹¶å°†æ—¥æœŸè½¬æ¢ä¸ºYYYY-MM-DDæ ¼å¼ã€‚
    å®ç°ä¸åŸå§‹Pythonè„šæœ¬ç›¸åŒçš„æ•°æ®è¿‡æ»¤é€»è¾‘ã€‚
    """
    print(f"Reading data from {file_path}...")
    df = pd.read_excel(file_path, sheet_name=sheet_name)

    print(f"  Original shape: {df.shape}")
    print(f"  Available columns: {list(df.columns)}")

    # æ ¹æ®æ–‡ä»¶ç±»å‹é‡å‘½ååˆ—
    if file_type == 'inbound':
        df.rename(columns={'å•æ®æ—¥æœŸ': 'record_date', 'ç‰©æ–™åç§°': 'product_name', 'ä¸»æ•°é‡': 'production_volume'}, inplace=True)
        print(f"  Mapped columns: record_date, product_name, production_volume")

        # ä¿®å¤ï¼šå°†äº§é‡ä»å…¬æ–¤(kg)è½¬æ¢ä¸ºå¨(t)
        if 'production_volume' in df.columns:
            df['production_volume'] = df['production_volume'] / 1000

        # åº”ç”¨ç”Ÿäº§æ•°æ®è¿‡æ»¤é€»è¾‘ï¼ˆå‚è€ƒåŸå§‹Pythonè„šæœ¬ï¼‰
        df = apply_production_data_filters(df)

    elif file_type == 'summary':
        # åº”ç”¨åº“å­˜æ•°æ®è¿‡æ»¤é€»è¾‘ï¼ˆå‚è€ƒåŸå§‹Pythonè„šæœ¬ï¼‰
        df = apply_inventory_data_filters(df)

        df.rename(columns={'ç‰©æ–™åç§°': 'product_name', 'ç»“å­˜': 'inventory_level'}, inplace=True)
        print(f"  Mapped columns: product_name, inventory_level")

        # åº“å­˜æ•°æ®ä½œä¸ºæœŸåˆåº“å­˜ï¼Œä¸éœ€è¦æ‰©å±•æ—¥æœŸ
        # å‡è®¾ 'æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx' æä¾›çš„æ˜¯è®¡ç®—å‘¨æœŸå¼€å§‹å‰ä¸€å¤©çš„æœŸæœ«åº“å­˜
        # ä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®ä»6æœˆ1æ—¥å¼€å§‹ï¼Œè¿™é‡Œåº”è¯¥æ˜¯5æœˆ31æ—¥çš„åº“å­˜
        # æˆ‘ä»¬å°†åœ¨ä¸»é€»è¾‘ä¸­å¤„ç†å®ƒï¼Œè¿™é‡Œåªåšé‡å‘½å
        print("  â„¹ï¸ Summary data will be used as starting inventory.")

    elif file_type == 'sales':
        # åº”ç”¨é”€å”®æ•°æ®è¿‡æ»¤é€»è¾‘ï¼ˆå‚è€ƒåŸå§‹Pythonè„šæœ¬ï¼‰
        df = apply_sales_data_filters(df)

        # æ£€æŸ¥å®é™…çš„ä»·æ ¼åˆ—å
        price_columns = [col for col in df.columns if 'å•ä»·' in col or 'ä»·æ ¼' in col]
        print(f"  Available price columns: {price_columns}")

        # å°è¯•å¤šç§ä»·æ ¼åˆ—æ˜ å°„
        if 'æœ¬å¸å«ç¨å•ä»·' in df.columns:
            df.rename(columns={'å‘ç¥¨æ—¥æœŸ': 'record_date', 'ç‰©æ–™åç§°': 'product_name', 'ä¸»æ•°é‡': 'sales_volume', 'æœ¬å¸å«ç¨å•ä»·': 'average_price'}, inplace=True)
            print(f"  Using 'æœ¬å¸å«ç¨å•ä»·' for price data")
        elif 'å«ç¨å•ä»·' in df.columns:
            df.rename(columns={'å‘ç¥¨æ—¥æœŸ': 'record_date', 'ç‰©æ–™åç§°': 'product_name', 'ä¸»æ•°é‡': 'sales_volume', 'å«ç¨å•ä»·': 'average_price'}, inplace=True)
            print(f"  Using 'å«ç¨å•ä»·' for price data")
        elif 'æœ¬å¸æ— ç¨å•ä»·' in df.columns and 'æœ¬å¸æ— ç¨é‡‘é¢' in df.columns:
            # è®¡ç®—å«ç¨ä»·æ ¼: (æ— ç¨é‡‘é¢ / ä¸»æ•°é‡) * 1.09
            df.rename(columns={'å‘ç¥¨æ—¥æœŸ': 'record_date', 'ç‰©æ–™åç§°': 'product_name', 'ä¸»æ•°é‡': 'sales_volume'}, inplace=True)
            # ä¿®å¤ï¼šä»·æ ¼å•ä½ä» å…ƒ/å…¬æ–¤ è½¬æ¢ä¸º å…ƒ/å¨
            df['average_price'] = (df['æœ¬å¸æ— ç¨é‡‘é¢'] / df['sales_volume']) * 1.09 * 1000
            print(f"  Calculated price from (æ— ç¨é‡‘é¢/ä¸»æ•°é‡*1.09*1000) to get price in Yuan/Ton")
        else:
            df.rename(columns={'å‘ç¥¨æ—¥æœŸ': 'record_date', 'ç‰©æ–™åç§°': 'product_name', 'ä¸»æ•°é‡': 'sales_volume'}, inplace=True)
            df['average_price'] = 0
            print(f"  âš ï¸  No suitable price column found, using 0")

    # ç¡®ä¿æ—¥æœŸæ ¼å¼ç»Ÿä¸€ä¸º 'YYYY-MM-DD'
    if 'record_date' in df.columns:
        # Convert to datetime, coercing errors to NaT
        df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce')
        # Drop rows where record_date is NaT (invalid date)
        before_count = len(df)
        df.dropna(subset=['record_date'], inplace=True)
        after_count = len(df)
        if before_count != after_count:
            print(f"  Dropped {before_count - after_count} rows with invalid dates")
        # Format to string
        df['record_date'] = df['record_date'].dt.strftime('%Y-%m-%d')
    else:
        print(f"  âš ï¸  Warning: No 'record_date' column found after renaming for {file_type} file.")

    # ç¡®ä¿äº§å“åç§°åˆ—å­˜åœ¨å¹¶å¤„ç†å¯èƒ½çš„å•†å“åç§°åˆ«å
    if 'product_name' not in df.columns and 'å•†å“åç§°' in df.columns:
        df.rename(columns={'å•†å“åç§°': 'product_name'}, inplace=True)

    # å¯¹æ‰€æœ‰å…³é”®æ•°å€¼åˆ—è¿›è¡Œéæ•°å€¼å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œå°† NaN æ›¿æ¢ä¸º 0ï¼‰
    numeric_cols = [
        'production_volume', 'sales_volume', 'inventory_level', 'average_price'
    ]
    for col in numeric_cols:
        if col in df.columns:
            before_conversion = df[col].isna().sum()
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
            after_conversion = (df[col] == 0).sum()
            print(f"  Column '{col}': {before_conversion} NaN values, {after_conversion} zero values after conversion")

    # --- å•ä½è½¬æ¢ ---
    # ä¿®å¤ï¼šå°†é”€é‡å’Œäº§é‡ä»å…¬æ–¤(kg)è½¬æ¢ä¸ºå¨(t)
    if 'sales_volume' in df.columns:
        print("  ğŸ”§ CONVERTING: sales_volume from KG to Tons")
        df['sales_volume'] = df['sales_volume'] / 1000
    if 'production_volume' in df.columns:
        print("  ğŸ”§ CONVERTING: production_volume from KG to Tons")
        df['production_volume'] = df['production_volume'] / 1000

    print(f"  Final shape: {df.shape}")
    return df

def calculate_inventory_turnover(df):
    """
    è®¡ç®—æ¯ä¸ªäº§å“çš„æ¯æ—¥åº“å­˜å‘¨è½¬å¤©æ•°ã€‚
    å…¬å¼ï¼šåº“å­˜å‘¨è½¬å¤©æ•° = å½“æ—¥ç»“å­˜åº“å­˜ / è¿‡å»30å¤©æ—¥å‡é”€å”®é‡
    """
    print("  ğŸ”„ Calculating inventory turnover days...")
    if df.empty or 'sales_volume' not in df.columns or 'inventory_level' not in df.columns:
        print("  âš ï¸ Not enough data to calculate turnover days. Skipping.")
        df['inventory_turnover_days'] = 0
        return df

    # ç¡®ä¿æ•°æ®æŒ‰äº§å“å’Œæ—¥æœŸæ’åº
    # å¦‚æœ record_date ä¸æ˜¯ datetime ç±»å‹ï¼Œå…ˆè½¬æ¢
    if not pd.api.types.is_datetime64_any_dtype(df['record_date']):
        df['record_date'] = pd.to_datetime(df['record_date'])
    
    df.sort_values(by=['product_id', 'record_date'], inplace=True)

    # è®¡ç®—è¿‡å»30å¤©çš„æ—¥å‡é”€å”®é‡
    # ä½¿ç”¨ rolling(window=30, min_periods=1) æ¥å¤„ç†æ•°æ®å¼€å§‹æ—¶ä¸è¶³30å¤©çš„æƒ…å†µ
    df['avg_sales_30d'] = df.groupby('product_id')['sales_volume'].transform(
        lambda x: x.rolling(window=30, min_periods=1).mean()
    )

    # è®¡ç®—åº“å­˜å‘¨è½¬å¤©æ•°
    # å¤„ç†æ—¥å‡é”€é‡ä¸º0çš„æƒ…å†µï¼Œé¿å…é™¤ä»¥é›¶
    df['inventory_turnover_days'] = df['inventory_level'] / df['avg_sales_30d']
    
    # å°†æ— ç©·å¤§ï¼ˆç”±äºé™¤ä»¥é›¶äº§ç”Ÿï¼‰å’ŒNaNå€¼æ›¿æ¢ä¸º0
    df['inventory_turnover_days'].replace([np.inf, -np.inf], 0, inplace=True)
    df.fillna({'inventory_turnover_days': 0}, inplace=True)

    print(f"  âœ… Calculated inventory turnover days.")
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    df.drop(columns=['avg_sales_30d'], inplace=True)
    
    return df

def main():
    # --- 1. è¯»å–å¹¶åˆå¹¶æ‰€æœ‰Excelæ•°æ® ---
    all_data_frames = {}
    for key, path in EXCEL_FILES.items():
        all_data_frames[key] = clean_and_load_excel(path, key)

    # é¦–å…ˆï¼Œä»æ‰€æœ‰æ•°æ®ä¸­æå–å”¯ä¸€çš„äº§å“åç§°
    unique_products = set()
    for df_key, df in all_data_frames.items():
        if 'product_name' in df.columns:
            unique_products.update(df['product_name'].unique())

    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å†…å­˜SQLiteæ•°æ®åº“è¿æ¥
    engine = create_engine(f'sqlite:///{DB_NAME}')
    conn = engine.connect()

    # --- 2. æå–å¹¶æ’å…¥å”¯ä¸€çš„äº§å“ä¿¡æ¯åˆ° Products è¡¨ ---
    print("Extracting unique products...")
    products_df = pd.DataFrame(list(unique_products), columns=['product_name'])
    # æ‰‹åŠ¨åˆ†é… product_idï¼Œä»1å¼€å§‹
    products_df['product_id'] = products_df.index + 1
    
    products_df.to_sql(DB_TABLE_PRODUCTS, conn, if_exists='replace', index=False)
    print(f"{len(products_df)} unique products written to '{DB_TABLE_PRODUCTS}' table.")

    # ç›´æ¥ä» products_df åˆ›å»ºæ˜ å°„ï¼Œå› ä¸º product_id å·²ç»æ˜ç¡®åˆ†é…
    product_name_to_id = products_df.set_index('product_name')['product_id'].to_dict()

    # --- 3. å‡†å¤‡æ¯æ—¥æŒ‡æ ‡æ•°æ®å¹¶å…³è”äº§å“ID ---
    print("Preparing daily metrics data using a production-centric merge strategy...")

    # 1. èšåˆç”Ÿäº§æ•°æ® (inbound)
    inbound_df = all_data_frames.get('inbound')
    if inbound_df is not None and not inbound_df.empty:
        production_daily = inbound_df.groupby(['record_date', 'product_name'])['production_volume'].sum().reset_index()
        print(f"  Aggregated production data: {production_daily.shape[0]} records")
    else:
        production_daily = pd.DataFrame(columns=['record_date', 'product_name', 'production_volume'])
        print("  No production data found.")

    # 2. èšåˆé”€å”®æ•°æ® (sales)
    sales_df = all_data_frames.get('sales')
    if sales_df is not None and not sales_df.empty:
        # è®¡ç®—åŠ æƒå¹³å‡ä»·æ ¼æ‰€éœ€çš„æ€»é‡‘é¢
        sales_df['total_amount'] = sales_df['sales_volume'] * sales_df['average_price']
        sales_daily = sales_df.groupby(['record_date', 'product_name']).agg(
            sales_volume=('sales_volume', 'sum'),
            total_amount=('total_amount', 'sum')
        ).reset_index()
        # è®¡ç®—æœ€ç»ˆçš„åŠ æƒå¹³å‡ä»·æ ¼
        sales_daily['average_price'] = sales_daily['total_amount'] / sales_daily['sales_volume']
        sales_daily.drop(columns=['total_amount'], inplace=True)
        print(f"  Aggregated sales data: {sales_daily.shape[0]} records")
    else:
        sales_daily = pd.DataFrame(columns=['record_date', 'product_name', 'sales_volume', 'average_price'])
        print("  No sales data found.")

    # 3. ä»¥ç”Ÿäº§æ•°æ®ä¸ºæ ¸å¿ƒï¼Œåˆå¹¶é”€å”®æ•°æ®
    # ä½¿ç”¨ outer join ä¿ç•™æ‰€æœ‰æœ‰ç”Ÿäº§æˆ–é”€å”®çš„è®°å½•ï¼Œåç»­å†æ ¹æ®ç”Ÿäº§è®°å½•è¿›è¡Œè¿‡æ»¤
    merged_df = pd.merge(
        production_daily,
        sales_daily,
        on=['record_date', 'product_name'],
        how='outer'
    )
    print(f"  Merged production and sales data (outer join): {merged_df.shape[0]} records")

    # å…³é”®ä¿®å¤ï¼šåªä¿ç•™æœ‰ç”Ÿäº§è®°å½•çš„æ•°æ®è¡Œ
    # è¿‡æ»¤æ‰ production_volume ä¸º NaN æˆ– 0 çš„æƒ…å†µ
    final_metrics_df = merged_df[merged_df['production_volume'].notna() & (merged_df['production_volume'] > 0)].copy()
    print(f"  Filtered for production records only: {final_metrics_df.shape[0]} records remain")

    # 4. å®ç°åŠ¨æ€åº“å­˜è®¡ç®—
    print("  ğŸ”„ Implementing dynamic inventory calculation...")
    summary_df = all_data_frames.get('summary')
    
    # å‡†å¤‡æœŸåˆåº“å­˜ (å‰ä¸€å¤©çš„ç»“å­˜)
    # å°† 'inventory_level' ä»å…¬æ–¤è½¬æ¢ä¸ºå¨
    if summary_df is not None and not summary_df.empty:
        summary_df['inventory_level'] = summary_df['inventory_level'] / 1000
        initial_inventory = summary_df.set_index('product_name')['inventory_level'].to_dict()
        print(f"  Loaded {len(initial_inventory)} initial inventory records (converted to tons).")
    else:
        initial_inventory = {}
        print("  âš ï¸ No summary data found, starting with zero inventory.")

    # ç¡®ä¿ 'record_date' æ˜¯ datetime ç±»å‹ï¼Œä»¥ä¾¿æ’åº
    final_metrics_df['record_date'] = pd.to_datetime(final_metrics_df['record_date'])
    final_metrics_df.sort_values(by=['product_name', 'record_date'], inplace=True)

    # å¡«å……NaNå€¼ä¸º0ï¼Œä»¥ä¾¿è®¡ç®—
    final_metrics_df.fillna({'production_volume': 0, 'sales_volume': 0}, inplace=True)

    # æŒ‰äº§å“é€æ—¥è®¡ç®—åº“å­˜
    calculated_inventory = []
    # ä½¿ç”¨ .groupby('product_name') ç¡®ä¿æˆ‘ä»¬æŒ‰äº§å“å¤„ç†æ•°æ®
    for product_name, group in final_metrics_df.groupby('product_name'):
        # è·å–è¯¥äº§å“çš„æœŸåˆåº“å­˜ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸º0
        last_day_inventory = initial_inventory.get(product_name, 0)
        
        # è¿­ä»£è¯¥äº§å“çš„æ¯ä¸€å¤©è®°å½•
        for index, row in group.iterrows():
            # å½“æ—¥åº“å­˜ = æ˜¨æ—¥åº“å­˜ + å½“æ—¥ç”Ÿäº§ - å½“æ—¥é”€å”®
            current_inventory = last_day_inventory + row['production_volume'] - row['sales_volume']
            calculated_inventory.append({
                'record_date': row['record_date'],
                'product_name': product_name,
                'inventory_level': current_inventory
            })
            # æ›´æ–°æ˜¨æ—¥åº“å­˜ä¸ºä»Šæ—¥åº“å­˜ï¼Œä¸ºä¸‹ä¸€å¤©åšå‡†å¤‡
            last_day_inventory = current_inventory

    # å°†è®¡ç®—å‡ºçš„åº“å­˜è½¬æ¢ä¸ºDataFrame
    if calculated_inventory:
        inventory_df = pd.DataFrame(calculated_inventory)
        print(f"  âœ… Calculated {len(inventory_df)} dynamic inventory records.")
        # å°†è®¡ç®—å‡ºçš„åº“å­˜åˆå¹¶å›ä¸»DataFrame
        final_metrics_df = pd.merge(
            final_metrics_df,
            inventory_df,
            on=['record_date', 'product_name'],
            how='left'
        )
        print(f"  Merged dynamic inventory data: {final_metrics_df.shape[0]} records")
    else:
        final_metrics_df['inventory_level'] = 0
        print("  No inventory calculated, setting inventory_level to 0.")


    # 5. å…³è”äº§å“IDå¹¶å¡«å……ç¼ºå¤±å€¼
    final_metrics_df['product_id'] = final_metrics_df['product_name'].map(product_name_to_id)
    
    # å¡«å……NaNå€¼ä¸º0
    fill_values = {
        'sales_volume': 0,
        'inventory_level': 0,
        'average_price': 0,
        'production_volume': 0
    }
    final_metrics_df.fillna(fill_values, inplace=True)

    # 6. è®¡ç®—åº“å­˜å‘¨è½¬å¤©æ•°
    final_metrics_df = calculate_inventory_turnover(final_metrics_df)
    
    # å°† record_date è½¬å›å­—ç¬¦ä¸²æ ¼å¼ä»¥ä¾¿å†™å…¥SQL
    if pd.api.types.is_datetime64_any_dtype(final_metrics_df['record_date']):
        final_metrics_df['record_date'] = final_metrics_df['record_date'].dt.strftime('%Y-%m-%d')

    # ç­›é€‰å¹¶é‡æ’æœ€ç»ˆåˆ—
    final_metrics_df = final_metrics_df[[
        'record_date', 'product_id', 'production_volume',
        'sales_volume', 'inventory_level', 'average_price',
        'inventory_turnover_days'
    ]]
    
    print(f"  Created {len(final_metrics_df)} final data records for DailyMetrics table.")

    # --- 4. ä»ä¸´æ—¶æ•°æ®åº“å¯¼å‡ºä¸º.sqlæ–‡ä»¶ ---
    print(f"Dumping database to {SQL_OUTPUT_FILE}...")
    with open(SQL_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        # æ·»åŠ  schema.sql çš„å†…å®¹åˆ°å¯¼å…¥è„šæœ¬ä¸­ï¼Œä»¥ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
        with open('backend/schema.sql', 'r', encoding='utf-8') as schema_f:
            f.write(schema_f.read())
            f.write('\n\n') # æ·»åŠ ç©ºè¡Œåˆ†éš”

        # Manually construct and write Products inserts with correct column order
        for index, row in products_df.iterrows():
            sql_values = (
                f"{row['product_id']},"
                f"'{row['product_name']}',"
                f"NULL,"  # sku
                f"NULL"   # category
            )
            f.write(f'INSERT INTO "Products" (product_id, product_name, sku, category) VALUES ({sql_values});\n')
        f.write('\n') # Add a newline after products inserts
        print(f"{len(products_df)} products written to SQL file.")

        # Manually construct and write DailyMetrics inserts
        if not final_metrics_df.empty:
            for index, row in final_metrics_df.iterrows():
                sql_values = (
                    f"'{row['record_date']}',"
                    f"{row['product_id']},"
                    f"{row['production_volume']},"
                    f"{row['sales_volume']},"
                    f"{row['inventory_level']},"
                    f"{row['average_price']},"
                    f"{row['inventory_turnover_days']}"
                )
                f.write(f'INSERT INTO "DailyMetrics" (record_date, product_id, production_volume, sales_volume, inventory_level, average_price, inventory_turnover_days) VALUES ({sql_values});\n')
            print(f"{len(final_metrics_df)} records written to '{DB_TABLE_METRICS}' table in SQL file.")
        else:
            print("No metrics data to write to SQL file.")


    print("SQL file generated successfully.")

    # --- 5. æ¸…ç†ä¸´æ—¶æ•°æ®åº“æ–‡ä»¶ ---
    conn.close()
    if os.path.exists(DB_NAME):
        os.remove(DB_NAME)
        print(f"Temporary database file '{DB_NAME}' removed.")

    print("Process complete. You can now use wrangler to execute the .sql file:")
    print(f"npx wrangler d1 execute <YOUR_DB_NAME> --remote --file=./{SQL_OUTPUT_FILE}")

if __name__ == "__main__":
    # First inspect the Excel files to understand their structure
    print("ğŸ” INSPECTING EXCEL FILES...")
    inspect_excel_files()

    # Ask user if they want to proceed with import
    proceed = input("\nğŸ“‹ Do you want to proceed with data import? (y/n): ").lower().strip()
    if proceed == 'y':
        main()
    else:
        print("Import cancelled.")
