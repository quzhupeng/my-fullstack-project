#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ
ç‰ˆæœ¬: 2.0
ä½œè€…: Kilo Code
æ—¥æœŸ: 2025-01-05

åŠŸèƒ½:
1. ç²¾ç»†åŒ–å››ç»´æ•°æ®è´¨é‡è¯„åˆ†ä½“ç³»
2. æ™ºèƒ½å¼‚å¸¸æ£€æµ‹ç®—æ³•
3. å®æ—¶è´¨é‡ç›‘æ§å’Œå‘Šè­¦
4. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
5. ä¸šåŠ¡è§„åˆ™æ™ºèƒ½éªŒè¯
"""

import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict, field
import json
import os
import warnings
from scipy import stats
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn not available, ML anomaly detection disabled")

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_data_quality_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class QualityDimension:
    """æ•°æ®è´¨é‡ç»´åº¦è¯„åˆ†"""
    dimension_name: str
    score: float
    weight: float
    sub_scores: Dict[str, float] = field(default_factory=dict)
    issues_count: int = 0
    critical_issues: int = 0

@dataclass
class AnomalyDetection:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""
    detection_method: str
    anomaly_type: str
    confidence: float
    affected_records: List[int]
    statistical_info: Dict[str, Any] = field(default_factory=dict)
    business_impact: str = "medium"

@dataclass
class EnhancedQualityIssue:
    """å¢å¼ºå‹æ•°æ®è´¨é‡é—®é¢˜"""
    issue_id: str
    issue_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    confidence: float  # 0-1, æ£€æµ‹ç½®ä¿¡åº¦
    description: str
    affected_records: int
    table_name: str
    column_name: str
    detection_time: str
    detection_method: str
    suggested_action: str
    business_impact: str
    auto_fixable: bool = False
    anomaly_details: Optional[AnomalyDetection] = None

@dataclass
class EnhancedQualityMetrics:
    """å¢å¼ºå‹æ•°æ®è´¨é‡æŒ‡æ ‡"""
    # å››ç»´è¯„åˆ†
    completeness: QualityDimension
    accuracy: QualityDimension
    consistency: QualityDimension
    validity: QualityDimension
    
    # æ•´ä½“æŒ‡æ ‡
    overall_score: float
    weighted_score: float
    quality_grade: str  # A+, A, B+, B, C+, C, D, F
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_records: int
    valid_records: int
    issue_count: int
    critical_issue_count: int
    
    # è¶‹åŠ¿ä¿¡æ¯
    score_trend: str  # "improving", "stable", "declining"
    historical_comparison: Dict[str, float] = field(default_factory=dict)

@dataclass
class EnhancedQualityReport:
    """å¢å¼ºå‹æ•°æ®è´¨é‡æŠ¥å‘Š"""
    report_id: str
    timestamp: str
    metrics: EnhancedQualityMetrics
    issues: List[EnhancedQualityIssue]
    anomalies: List[AnomalyDetection]
    recommendations: List[str]
    data_sources: List[str]
    processing_time: float
    quality_summary: Dict[str, Any] = field(default_factory=dict)
    alert_level: str = "normal"  # "normal", "warning", "critical"

class EnhancedDataQualityMonitor:
    """å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self, excel_folder: str = './Excelæ–‡ä»¶å¤¹/', db_path: str = None):
        self.excel_folder = excel_folder
        self.db_path = db_path
        self.logger = logging.getLogger(f"{__name__}.EnhancedDataQualityMonitor")
        
        # å¢å¼ºå‹è´¨é‡é˜ˆå€¼é…ç½®
        self.quality_thresholds = {
            'A+': 0.98,  # å“è¶Š
            'A': 0.95,   # ä¼˜ç§€
            'B+': 0.90,  # è‰¯å¥½+
            'B': 0.85,   # è‰¯å¥½
            'C+': 0.75,  # å¯æ¥å—+
            'C': 0.70,   # å¯æ¥å—
            'D': 0.50,   # è¾ƒå·®
            'F': 0.0     # ä¸åˆæ ¼
        }
        
        # å››ç»´æƒé‡é…ç½®ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
        self.dimension_weights = {
            'completeness': 0.25,  # å®Œæ•´æ€§ 25%
            'accuracy': 0.40,      # å‡†ç¡®æ€§ 40%
            'consistency': 0.25,   # ä¸€è‡´æ€§ 25%
            'validity': 0.10       # æœ‰æ•ˆæ€§ 10%
        }
        
        # å¼‚å¸¸æ£€æµ‹å‚æ•°
        self.anomaly_params = {
            'z_score_threshold': 3.0,
            'iqr_multiplier': 1.5,
            'isolation_forest_contamination': 0.1,
            'confidence_threshold': 0.8
        }
        
        # æ•°æ®æºé…ç½®
        self.data_sources = {
            'sales': 'é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx',
            'inventory': 'æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx',
            'production': 'äº§æˆå“å…¥åº“åˆ—è¡¨.xlsx'
        }
        
        # ä¸šåŠ¡è§„åˆ™é…ç½®
        self.business_rules = {
            'product_filters': {
                'exclude_patterns': ['é²œ'],
                'include_patterns': ['å‡¤è‚ ', 'çƒ¤è‚ ', 'ç«è…¿è‚ ']
            },
            'required_fields': {
                'sales': ['ç‰©æ–™åç§°', 'ä¸»æ•°é‡', 'å•ä»·'],
                'inventory': ['ç‰©æ–™åç§°', 'å…¥åº“', 'å‡ºåº“'],
                'production': ['ç‰©æ–™åç§°', 'ä¸»æ•°é‡']
            },
            'value_ranges': {
                'ä¸»æ•°é‡': (0, 1000000),
                'å•ä»·': (0, 10000),
                'å…¥åº“': (0, 1000000),
                'å‡ºåº“': (0, 1000000)
            }
        }
        
        # å†å²æ•°æ®å­˜å‚¨
        self.quality_history = []
    
    def run_enhanced_quality_check(self) -> EnhancedQualityReport:
        """è¿è¡Œå¢å¼ºå‹æ•°æ®è´¨é‡æ£€æŸ¥"""
        start_time = datetime.now()
        self.logger.info("å¼€å§‹å¢å¼ºå‹æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        try:
            # 1. åŠ è½½æ‰€æœ‰æ•°æ®æº
            datasets = self._load_all_datasets()
            
            # 2. æ‰§è¡Œå¤šå±‚æ¬¡è´¨é‡æ£€æŸ¥
            all_issues = []
            all_anomalies = []
            dimension_scores = {}
            
            for source_name, df in datasets.items():
                if df is not None and not df.empty:
                    # åŸºç¡€è´¨é‡æ£€æŸ¥
                    issues, dimensions = self._check_enhanced_dataset_quality(df, source_name)
                    all_issues.extend(issues)
                    dimension_scores[source_name] = dimensions
                    
                    # å¼‚å¸¸æ£€æµ‹
                    anomalies = self._detect_anomalies(df, source_name)
                    all_anomalies.extend(anomalies)
            
            # 3. è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡
            overall_metrics = self._calculate_enhanced_metrics(dimension_scores, all_issues)
            
            # 4. ç”Ÿæˆæ™ºèƒ½å»ºè®®
            recommendations = self._generate_intelligent_recommendations(
                all_issues, all_anomalies, overall_metrics
            )
            
            # 5. ç¡®å®šå‘Šè­¦çº§åˆ«
            alert_level = self._determine_alert_level(overall_metrics, all_issues)
            
            # 6. åˆ›å»ºå¢å¼ºå‹æŠ¥å‘Š
            processing_time = (datetime.now() - start_time).total_seconds()
            report = EnhancedQualityReport(
                report_id=f"EQR_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                timestamp=datetime.now().isoformat(),
                metrics=overall_metrics,
                issues=all_issues,
                anomalies=all_anomalies,
                recommendations=recommendations,
                data_sources=list(datasets.keys()),
                processing_time=processing_time,
                quality_summary=self._generate_quality_summary(overall_metrics, all_issues),
                alert_level=alert_level
            )
            
            # 7. å­˜å‚¨å†å²æ•°æ®
            self._store_quality_history(report)
            
            self.logger.info(f"å¢å¼ºå‹æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            return report
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºå‹æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            raise
    
    def _load_all_datasets(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        datasets = {}
        
        for source_name, filename in self.data_sources.items():
            try:
                filepath = os.path.join(self.excel_folder, filename)
                if os.path.exists(filepath):
                    df = pd.read_excel(filepath)
                    datasets[source_name] = df
                    self.logger.info(f"åŠ è½½ {source_name} æ•°æ®: {len(df)} æ¡è®°å½•")
                else:
                    self.logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    datasets[source_name] = None
            except Exception as e:
                self.logger.error(f"åŠ è½½ {source_name} æ•°æ®å¤±è´¥: {e}")
                datasets[source_name] = None
        
        return datasets
    
    def _check_enhanced_dataset_quality(self, df: pd.DataFrame, source_name: str) -> Tuple[List[EnhancedQualityIssue], Dict[str, QualityDimension]]:
        """æ£€æŸ¥å•ä¸ªæ•°æ®é›†çš„å¢å¼ºå‹è´¨é‡"""
        self.logger.info(f"æ£€æŸ¥ {source_name} å¢å¼ºå‹æ•°æ®è´¨é‡...")
        
        issues = []
        
        # 1. å®Œæ•´æ€§æ£€æŸ¥ï¼ˆ25%æƒé‡ï¼‰
        completeness_issues, completeness_dim = self._check_enhanced_completeness(df, source_name)
        issues.extend(completeness_issues)
        
        # 2. å‡†ç¡®æ€§æ£€æŸ¥ï¼ˆ40%æƒé‡ï¼‰
        accuracy_issues, accuracy_dim = self._check_enhanced_accuracy(df, source_name)
        issues.extend(accuracy_issues)
        
        # 3. ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆ25%æƒé‡ï¼‰
        consistency_issues, consistency_dim = self._check_enhanced_consistency(df, source_name)
        issues.extend(consistency_issues)
        
        # 4. æœ‰æ•ˆæ€§æ£€æŸ¥ï¼ˆ10%æƒé‡ï¼‰
        validity_issues, validity_dim = self._check_enhanced_validity(df, source_name)
        issues.extend(validity_issues)
        
        dimensions = {
            'completeness': completeness_dim,
            'accuracy': accuracy_dim,
            'consistency': consistency_dim,
            'validity': validity_dim
        }
        
        return issues, dimensions
    
    def _check_enhanced_completeness(self, df: pd.DataFrame, source_name: str) -> Tuple[List[EnhancedQualityIssue], QualityDimension]:
        """å¢å¼ºå‹å®Œæ•´æ€§æ£€æŸ¥"""
        issues = []
        sub_scores = {}
        
        # 1. ç©ºå€¼æ£€æµ‹
        null_counts = df.isnull().sum()
        total_cells = len(df) * len(df.columns)
        null_cells = null_counts.sum()
        
        null_ratio = null_cells / total_cells if total_cells > 0 else 0
        sub_scores['null_ratio'] = max(0, 1 - null_ratio)
        
        # 2. å¿…å¡«å­—æ®µæ£€æŸ¥
        required_fields = self.business_rules.get('required_fields', {}).get(source_name, [])
        required_completeness = 1.0
        
        for field in required_fields:
            if field in df.columns:
                field_null_ratio = df[field].isnull().sum() / len(df)
                if field_null_ratio > 0:
                    severity = self._determine_enhanced_severity(field_null_ratio, 'completeness')
                    confidence = 1.0 - field_null_ratio
                    
                    issues.append(EnhancedQualityIssue(
                        issue_id=f"COMP_{source_name}_{field}_{datetime.now().strftime('%H%M%S')}",
                        issue_type="missing_required_data",
                        severity=severity,
                        confidence=confidence,
                        description=f"å¿…å¡«å­—æ®µ '{field}' æœ‰ {df[field].isnull().sum()} ä¸ªç©ºå€¼ ({field_null_ratio:.1%})",
                        affected_records=df[field].isnull().sum(),
                        table_name=source_name,
                        column_name=field,
                        detection_time=datetime.now().isoformat(),
                        detection_method="required_field_validation",
                        suggested_action=f"æ£€æŸ¥ {field} å­—æ®µçš„æ•°æ®å½•å…¥æµç¨‹ï¼Œç¡®ä¿å¿…å¡«å­—æ®µå®Œæ•´æ€§",
                        business_impact="high" if field_null_ratio > 0.1 else "medium",
                        auto_fixable=False
                    ))
                    required_completeness *= (1 - field_null_ratio)
        
        sub_scores['required_fields'] = required_completeness
        
        # 3. æ•°æ®è¦†ç›–åº¦æ£€æŸ¥
        coverage_score = 1.0
        if 'ç‰©æ–™åç§°' in df.columns:
            unique_products = df['ç‰©æ–™åç§°'].nunique()
            total_records = len(df)
            coverage_ratio = unique_products / total_records if total_records > 0 else 0
            coverage_score = min(1.0, coverage_ratio * 2)
        
        sub_scores['data_coverage'] = coverage_score
        
        # è®¡ç®—å®Œæ•´æ€§æ€»åˆ†
        completeness_score = (
            sub_scores['null_ratio'] * 0.4 +
            sub_scores['required_fields'] * 0.4 +
            sub_scores['data_coverage'] * 0.2
        )
        
        dimension = QualityDimension(
            dimension_name="completeness",
            score=completeness_score,
            weight=self.dimension_weights['completeness'],
            sub_scores=sub_scores,
            issues_count=len([i for i in issues if 'missing' in i.issue_type]),
            critical_issues=len([i for i in issues if i.severity == 'critical' and 'missing' in i.issue_type])
        )
        
        return issues, dimension
    
    def _check_enhanced_accuracy(self, df: pd.DataFrame, source_name: str) -> Tuple[List[EnhancedQualityIssue], QualityDimension]:
        """å¢å¼ºå‹å‡†ç¡®æ€§æ£€æŸ¥"""
        issues = []
        sub_scores = {}
        
        # 1. æ•°å€¼èŒƒå›´éªŒè¯
        range_violations = 0
        total_numeric_checks = 0
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for column in numeric_columns:
            if column in self.business_rules['value_ranges']:
                min_val, max_val = self.business_rules['value_ranges'][column]
                out_of_range = ((df[column] < min_val) | (df[column] > max_val)).sum()
                total_numeric_checks += len(df)
                range_violations += out_of_range
                
                if out_of_range > 0:
                    violation_ratio = out_of_range / len(df)
                    severity = self._determine_enhanced_severity(violation_ratio, 'accuracy')
                    
                    issues.append(EnhancedQualityIssue(
                        issue_id=f"ACC_RANGE_{source_name}_{column}_{datetime.now().strftime('%H%M%S')}",
                        issue_type="value_out_of_range",
                        severity=severity,
                        confidence=0.95,
                        description=f"åˆ— '{column}' æœ‰ {out_of_range} ä¸ªå€¼è¶…å‡ºåˆç†èŒƒå›´ [{min_val}, {max_val}]",
                        affected_records=out_of_range,
                        table_name=source_name,
                        column_name=column,
                        detection_time=datetime.now().isoformat(),
                        detection_method="business_rule_validation",
                        suggested_action=f"æ£€æŸ¥ {column} åˆ—çš„æ•°æ®å½•å…¥ï¼Œç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…",
                        business_impact="high" if violation_ratio > 0.05 else "medium",
                        auto_fixable=True
                    ))
        
        sub_scores['range_validation'] = max(0, (total_numeric_checks - range_violations) / total_numeric_checks) if total_numeric_checks > 0 else 1.0
        
        # 2. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        statistical_score = self._check_statistical_accuracy(df, source_name, issues)
        sub_scores['statistical_accuracy'] = statistical_score
        
        # 3. ä¸šåŠ¡é€»è¾‘éªŒè¯
        business_logic_score = self._check_business_logic_accuracy(df, source_name, issues)
        sub_scores['business_logic'] = business_logic_score
        
        # è®¡ç®—å‡†ç¡®æ€§æ€»åˆ†
        accuracy_score = (
            sub_scores['range_validation'] * 0.4 +
            sub_scores['statistical_accuracy'] * 0.4 +
            sub_scores['business_logic'] * 0.2
        )
        
        dimension = QualityDimension(
            dimension_name="accuracy",
            score=accuracy_score,
            weight=self.dimension_weights['accuracy'],
            sub_scores=sub_scores,
            issues_count=len([i for i in issues if 'accuracy' in i.detection_method or 'range' in i.issue_type]),
            critical_issues=len([i for i in issues if i.severity == 'critical' and ('accuracy' in i.detection_method or 'range' in i.issue_type)])
        )
        
        return issues, dimension
    
    def _check_statistical_accuracy(self, df: pd.DataFrame, source_name: str, issues: List[EnhancedQualityIssue]) -> float:
        """ç»Ÿè®¡å‡†ç¡®æ€§æ£€æŸ¥"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) == 0:
            return 1.0
        
        outlier_ratio = 0
        total_values = 0
        
        for column in numeric_columns:
            if df[column].notna().sum() > 10:
                values = df[column].dropna()
                total_values += len(values)
                
                # Z-Scoreå¼‚å¸¸æ£€æµ‹
                z_scores = np.abs(stats.zscore(values))
                z_outliers = (z_scores > self.anomaly_params['z_score_threshold']).sum()
                
                # IQRå¼‚å¸¸æ£€æµ‹
                Q1 = values.quantile(0.25)
                Q3 = values.quantile(0.75)
                IQR = Q3 - Q1
                iqr_outliers = ((values < (Q1 - 1.5 * IQR)) | (values > (Q3 + 1.5 * IQR))).sum()
                
                # å–è¾ƒä¿å®ˆçš„ç»“æœ
                outliers = min(z_outliers, iqr_outliers)
                outlier_ratio += outliers
                
                if outliers > 0:
                    outlier_percentage = outliers / len(values)
                    if outlier_percentage > 0.05:
                        severity = self._determine_enhanced_severity(outlier_percentage, 'accuracy')
                        
                        issues.append(EnhancedQualityIssue(
                            issue_id=f"ACC_STAT_{source_name}_{column}_{datetime.now().strftime('%H%M%S')}",
                            issue_type="statistical_outlier",
                            severity=severity,
                            confidence=0.85,
                            description=f"åˆ— '{column}' æ£€æµ‹åˆ° {outliers} ä¸ªç»Ÿè®¡å¼‚å¸¸å€¼ ({outlier_percentage:.1%})",
                            affected_records=outliers,
                            table_name=source_name,
                            column_name=column,
                            detection_time=datetime.now().isoformat(),
                            detection_method="statistical_analysis",
                            suggested_action=f"æ£€æŸ¥ {column} åˆ—çš„å¼‚å¸¸å€¼ï¼Œç¡®è®¤æ˜¯å¦ä¸ºæ•°æ®å½•å…¥é”™è¯¯",
                            business_impact="medium",
                            auto_fixable=False
                        ))
        
        return max(0, (total_values - outlier_ratio) / total_values) if total_values > 0 else 1.0
    
    def _check_business_logic_accuracy(self, df: pd.DataFrame, source_name: str, issues: List[EnhancedQualityIssue]) -> float:
        """ä¸šåŠ¡é€»è¾‘å‡†ç¡®æ€§æ£€æŸ¥"""
        violations = 0
        total_checks = 0
        
        # æ£€æŸ¥äº§å“è¿‡æ»¤è§„åˆ™
        if 'ç‰©æ–™åç§°' in df.columns:
            exclude_patterns = self.business_rules['product_filters']['exclude_patterns']
            include_patterns = self.business_rules['product_filters']['include_patterns']
            
            for pattern in exclude_patterns:
                excluded_products = df[df['ç‰©æ–™åç§°'].str.contains(pattern, na=False)]
                if len(excluded_products) > 0:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¾‹å¤–æƒ…å†µ
                    exceptions = excluded_products[
                        excluded_products['ç‰©æ–™åç§°'].str.contains('|'.join(include_patterns), na=False)
                    ]
                    actual_violations = len(excluded_products) - len(exceptions)
                    
                    if actual_violations > 0:
                        violations += actual_violations
                        violation_ratio = actual_violations / len(df)
                        
                        issues.append(EnhancedQualityIssue(
                            issue_id=f"ACC_BIZ_{source_name}_{pattern}_{datetime.now().strftime('%H%M%S')}",
                            issue_type="business_rule_violation",
                            severity=self._determine_enhanced_severity(violation_ratio, 'accuracy'),
                            confidence=0.9,
                            description=f"å‘ç° {actual_violations} ä¸ªåº”è¢«è¿‡æ»¤çš„ '{pattern}' ç±»äº§å“",
                            affected_records=actual_violations,
                            table_name=source_name,
                            column_name="ç‰©æ–™åç§°",
                            detection_time=datetime.now().isoformat(),
                            detection_method="business_rule_validation",
                            suggested_action=f"æ£€æŸ¥äº§å“è¿‡æ»¤è§„åˆ™ï¼Œç¡®ä¿ '{pattern}' ç±»äº§å“è¢«æ­£ç¡®å¤„ç†",
                            business_impact="medium",
                            auto_fixable=True
                        ))
            
            total_checks += len(df)
        
        return max(0, (total_checks - violations) / total_checks) if total_checks > 0 else 1.0
    
    def _check_enhanced_consistency(self, df: pd.DataFrame, source_name: str) -> Tuple[List[EnhancedQualityIssue], QualityDimension]:
        """å¢å¼ºå‹ä¸€è‡´æ€§æ£€æŸ¥"""
        issues = []
        sub_scores = {}
        
        # 1. é‡å¤è®°å½•æ£€æµ‹
        duplicate_count = df.duplicated().sum()
        duplicate_ratio = duplicate_count / len(df) if len(df) > 0 else 0
        sub_scores['duplicate_records'] = max(0, 1 - duplicate_ratio)
        
        if duplicate_count > 0:
            severity = self._determine_enhanced_severity(duplicate_ratio, 'consistency')
            
            issues.append(EnhancedQualityIssue(
                issue_id=f"CONS_DUP_{source_name}_{datetime.now().strftime('%H%M%S')}",
                issue_type="duplicate_records",
                severity=severity,
                confidence=1.0,
                description=f"å‘ç° {duplicate_count} æ¡é‡å¤è®°å½• ({duplicate_ratio:.1%})",
                affected_records=duplicate_count,
                table_name=source_name,
                column_name="all",
                detection_time=datetime.now().isoformat(),
                detection_method="duplicate_detection",
                suggested_action="åˆ é™¤é‡å¤è®°å½•æˆ–æ£€æŸ¥æ•°æ®å½•å…¥æµç¨‹",
                business_impact="medium" if duplicate_ratio > 0.05 else "low",
                auto_fixable=True
            ))
        
        # 2. æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥
        format_consistency_score = 1.0
        sub_scores['format_consistency'] = format_consistency_score
        
        # 3. å…³è”æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        relational_consistency_score = 1.0
        sub_scores['relational_consistency'] = relational_consistency_score
        
        # è®¡ç®—ä¸€è‡´æ€§æ€»åˆ†
        consistency_score = (
            sub_scores['duplicate_records'] * 0.4 +
            sub_scores['format_consistency'] * 0.3 +
            sub_scores['relational_consistency'] * 0.3
        )
        
        dimension = QualityDimension(
            dimension_name="consistency",
            score=consistency_score,
            weight=self.dimension_weights['consistency'],
            sub_scores=sub_scores,
            issues_count=len([i for i in issues if 'consistency' in i.detection_method or 'duplicate' in i.issue_type]),
            critical_issues=len([i for i in issues if i.severity == 'critical' and ('consistency' in i.detection_method or 'duplicate' in i.issue_type)])
        )
        
        return issues, dimension
    
    def _check_enhanced_validity(self, df: pd.DataFrame, source_name: str) -> Tuple[List[EnhancedQualityIssue], QualityDimension]:
        """å¢å¼ºå‹æœ‰æ•ˆæ€§æ£€æŸ¥"""
        issues = []
        sub_scores = {}
        
        # 1. ä¸šåŠ¡è§„åˆ™æœ‰æ•ˆæ€§
        business_validity_score = 1.0
        sub_scores['business_validity'] = business_validity_score
        
        # 2. æ—¶é—´åºåˆ—æœ‰æ•ˆæ€§
        temporal_validity_score = 1.0
        sub_scores['temporal_validity'] = temporal_validity_score
        
        # 3. æ•°æ®ç±»å‹æœ‰æ•ˆæ€§
        type_validity_score = 1.0
        sub_scores['type_validity'] = type_validity_score
        
        # è®¡ç®—æœ‰æ•ˆæ€§æ€»åˆ†
        validity_score = (
            sub_scores['business_validity'] * 0.5 +
            sub_scores['temporal_validity'] * 0.3 +
            sub_scores['type_validity'] * 0.2
        )
        
        dimension = QualityDimension(
            dimension_name="validity",
            score=validity_score,
            weight=self.dimension_weights['validity'],
            sub_scores=sub_scores,
            issues_count=len([i for i in issues if 'validity' in i.detection_method]),
            critical_issues=len([i for i in issues if i.severity == 'critical' and 'validity' in i.detection_method])
        )
        
        return issues, dimension
    
    def _detect_anomalies(self, df: pd.DataFrame, source_name: str) -> List[AnomalyDetection]:
        """æ™ºèƒ½å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        
        # 1. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        statistical_anomalies = self._detect_statistical_anomalies(df, source_name)
        anomalies.extend(statistical_anomalies)
        
        # 2. æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
        if SKLEARN_AVAILABLE:
            ml_anomalies = self._detect_ml_anomalies(df, source_name)
            anomalies.extend(ml_anomalies)
        
        return anomalies
    
    def _detect_statistical_anomalies(self, df: pd.DataFrame, source_name: str) -> List[AnomalyDetection]:
        """ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for column in numeric_columns:
            if df[column].notna().sum() > 10:
                values = df[column].dropna()
                
                # Z-Scoreå¼‚å¸¸æ£€æµ‹
                z_scores = np.abs(stats.zscore(values))
                z_outliers_mask = z_scores > self.anomaly_params['z_score_threshold']
                z_outliers_indices = values[z_outliers_mask].index.tolist()
                
                if len(z_outliers_indices) > 0:
                    confidence = min(0.95, (z_scores[z_outliers_mask].mean() - self.anomaly_params['z_score_threshold']) / 3.0)
                    
                    anomalies.append(AnomalyDetection(
                        detection_method="z_score",
                        anomaly_type="statistical_outlier",
                        confidence=confidence,
                        affected_records=z_outliers_indices,
                        statistical_info={
                            'column': column,
                            'mean': float(values.mean()),
                            'std': float(values.std()),
                            'z_threshold': self.anomaly_params['z_score_threshold'],
                            'outlier_count': len(z_outliers_indices)
                        },
                        business_impact=self._assess_business_impact(column, len(z_outliers_indices), len(values))
                    ))
        
        return anomalies
    
    def _detect_ml_anomalies(self, df: pd.DataFrame, source_name: str) -> List[AnomalyDetection]:
        """æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹"""
        anomalies = []
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_columns) >= 2:
            try:
                # å‡†å¤‡æ•°æ®
                data = df[numeric_columns].dropna()
                if len(data) > 20:
                    scaler = StandardScaler()
                    scaled_data = scaler.fit_transform(data)
                    
                    # Isolation Forestå¼‚å¸¸æ£€æµ‹
                    iso_forest = IsolationForest(
                        contamination=self.anomaly_params['isolation_forest_contamination'],
                        random_state=42
                    )
                    outliers = iso_forest.fit_predict(scaled_data)
                    outlier_indices = data.index[outliers == -1].tolist()
                    
                    if len(outlier_indices) > 0:
                        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
                        anomaly_scores = iso_forest.decision_function(scaled_data)
                        confidence = float(np.mean(np.abs(anomaly_scores[outliers == -1])))
                        
                        anomalies.append(AnomalyDetection(
                            detection_method="isolation_forest",
                            anomaly_type="multivariate_outlier",
                            confidence=min(0.95, confidence),
                            affected_records=outlier_indices,
                            statistical_info={
                                'features': list(numeric_columns),
                                'contamination': self.anomaly_params['isolation_forest_contamination'],
                                'outlier_count': len(outlier_indices),
                                'total_records': len(data)
                            },
                            business_impact=self._assess_business_impact('multivariate', len(outlier_indices), len(data))
                        ))
            except Exception as e:
                self.logger.warning(f"æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        
        return anomalies
    
    def _assess_business_impact(self, column: str, outlier_count: int, total_count: int) -> str:
        """è¯„ä¼°ä¸šåŠ¡å½±å“"""
        impact_ratio = outlier_count / total_count if total_count > 0 else 0
        
        if impact_ratio > 0.1:
            return "high"
        elif impact_ratio > 0.05:
            return "medium"
        else:
            return "low"
    
    def _determine_enhanced_severity(self, ratio: float, dimension: str) -> str:
        """ç¡®å®šå¢å¼ºå‹ä¸¥é‡ç¨‹åº¦"""
        # æ ¹æ®ä¸åŒç»´åº¦è°ƒæ•´é˜ˆå€¼
        thresholds = {
            'completeness': {'critical': 0.2, 'high': 0.1, 'medium': 0.05},
            'accuracy': {'critical': 0.15, 'high': 0.08, 'medium': 0.03},
            'consistency': {'critical': 0.1, 'high': 0.05, 'medium': 0.02},
            'validity': {'critical': 0.05, 'high': 0.02, 'medium': 0.01}
        }
        
        dim_thresholds = thresholds.get(dimension, thresholds['accuracy'])
        
        if ratio >= dim_thresholds['critical']:
            return 'critical'
        elif ratio >= dim_thresholds['high']:
            return 'high'
        elif ratio >= dim_thresholds['medium']:
            return 'medium'
        else:
            return 'low'
    
    def _calculate_enhanced_metrics(self, dimension_scores: Dict[str, Dict[str, QualityDimension]],
                                  all_issues: List[EnhancedQualityIssue]) -> EnhancedQualityMetrics:
        """è®¡ç®—å¢å¼ºå‹è´¨é‡æŒ‡æ ‡"""
        # èšåˆå„æ•°æ®æºçš„ç»´åº¦åˆ†æ•°
        aggregated_dimensions = {}
        
        for dimension_name in ['completeness', 'accuracy', 'consistency', 'validity']:
            scores = []
            weights = []
            total_issues = 0
            critical_issues = 0
            
            for source_name, dimensions in dimension_scores.items():
                if dimension_name in dimensions:
                    dim = dimensions[dimension_name]
                    scores.append(dim.score)
                    weights.append(1.0)  # å¯ä»¥æ ¹æ®æ•°æ®æºé‡è¦æ€§è°ƒæ•´æƒé‡
                    total_issues += dim.issues_count
                    critical_issues += dim.critical_issues
            
            if scores:
                weighted_score = np.average(scores, weights=weights)
                aggregated_dimensions[dimension_name] = QualityDimension(
                    dimension_name=dimension_name,
                    score=weighted_score,
                    weight=self.dimension_weights[dimension_name],
                    sub_scores={},
                    issues_count=total_issues,
                    critical_issues=critical_issues
                )
            else:
                aggregated_dimensions[dimension_name] = QualityDimension(
                    dimension_name=dimension_name,
                    score=1.0,
                    weight=self.dimension_weights[dimension_name],
                    sub_scores={},
                    issues_count=0,
                    critical_issues=0
                )
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        weighted_score = sum(
            dim.score * dim.weight
            for dim in aggregated_dimensions.values()
        )
        
        # ç¡®å®šè´¨é‡ç­‰çº§
        quality_grade = self._determine_quality_grade(weighted_score)
        
        # è®¡ç®—è¶‹åŠ¿
        score_trend = self._calculate_score_trend(weighted_score)
        
        # ç»Ÿè®¡ä¿¡æ¯ - ä¿®å¤ï¼šall_issuesæ˜¯EnhancedQualityIssueå¯¹è±¡åˆ—è¡¨ï¼Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
        total_records = 0
        for source_name, dimensions in dimension_scores.items():
            for dim_name, dim in dimensions.items():
                total_records += dim.issues_count
        
        critical_issue_count = len([i for i in all_issues if i.severity == 'critical'])
        
        return EnhancedQualityMetrics(
            completeness=aggregated_dimensions['completeness'],
            accuracy=aggregated_dimensions['accuracy'],
            consistency=aggregated_dimensions['consistency'],
            validity=aggregated_dimensions['validity'],
            overall_score=weighted_score,
            weighted_score=weighted_score,
            quality_grade=quality_grade,
            total_records=total_records,
            valid_records=max(0, total_records - len(all_issues)),
            issue_count=len(all_issues),
            critical_issue_count=critical_issue_count,
            score_trend=score_trend,
            historical_comparison=self._get_historical_comparison(weighted_score)
        )
    
    def _determine_quality_grade(self, score: float) -> str:
        """ç¡®å®šè´¨é‡ç­‰çº§"""
        for grade, threshold in self.quality_thresholds.items():
            if score >= threshold:
                return grade
        return 'F'
    
    def _calculate_score_trend(self, current_score: float) -> str:
        """è®¡ç®—åˆ†æ•°è¶‹åŠ¿"""
        if len(self.quality_history) < 2:
            return "stable"
        
        recent_scores = [h.metrics.weighted_score for h in self.quality_history[-3:]]
        recent_scores.append(current_score)
        
        if len(recent_scores) >= 3:
            trend = np.polyfit(range(len(recent_scores)), recent_scores, 1)[0]
            if trend > 0.01:
                return "improving"
            elif trend < -0.01:
                return "declining"
        
        return "stable"
    
    def _get_historical_comparison(self, current_score: float) -> Dict[str, float]:
        """è·å–å†å²å¯¹æ¯”æ•°æ®"""
        comparison = {}
        
        if self.quality_history:
            last_score = self.quality_history[-1].metrics.weighted_score
            comparison['last_period'] = current_score - last_score
            
            if len(self.quality_history) >= 7:
                week_ago_score = self.quality_history[-7].metrics.weighted_score
                comparison['week_ago'] = current_score - week_ago_score
            
            if len(self.quality_history) >= 30:
                month_ago_score = self.quality_history[-30].metrics.weighted_score
                comparison['month_ago'] = current_score - month_ago_score
        
        return comparison
    
    def _generate_intelligent_recommendations(self, issues: List[EnhancedQualityIssue],
                                            anomalies: List[AnomalyDetection],
                                            metrics: EnhancedQualityMetrics) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        recommendations = []
        
        # 1. åŸºäºè´¨é‡ç­‰çº§çš„å»ºè®®
        if metrics.quality_grade in ['F', 'D']:
            recommendations.append("ğŸš¨ æ•°æ®è´¨é‡ä¸¥é‡ä¸è¾¾æ ‡ï¼Œå»ºè®®ç«‹å³å¯åŠ¨æ•°æ®è´¨é‡æ”¹è¿›è®¡åˆ’")
            recommendations.append("ğŸ“‹ ä¼˜å…ˆå¤„ç†æ‰€æœ‰criticalçº§åˆ«çš„æ•°æ®è´¨é‡é—®é¢˜")
        elif metrics.quality_grade in ['C', 'C+']:
            recommendations.append("âš ï¸ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®åˆ¶å®šç³»ç»Ÿæ€§çš„è´¨é‡æå‡æ–¹æ¡ˆ")
        elif metrics.quality_grade in ['B', 'B+']:
            recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå¹¶ä¼˜åŒ–ç»†èŠ‚é—®é¢˜")
        else:
            recommendations.append("ğŸŒŸ æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå»ºè®®å»ºç«‹æœ€ä½³å®è·µæ ‡å‡†")
        
        # 2. åŸºäºç»´åº¦åˆ†æ•°çš„å»ºè®®
        if metrics.completeness.score < 0.8:
            recommendations.append("ğŸ“Š å®Œæ•´æ€§é—®é¢˜çªå‡ºï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å½•å…¥æµç¨‹å’Œå¿…å¡«å­—æ®µéªŒè¯")
        
        if metrics.accuracy.score < 0.8:
            recommendations.append("ğŸ¯ å‡†ç¡®æ€§éœ€è¦æå‡ï¼Œå»ºè®®åŠ å¼ºæ•°æ®éªŒè¯è§„åˆ™å’Œå¼‚å¸¸å€¼æ£€æµ‹")
        
        if metrics.consistency.score < 0.8:
            recommendations.append("ğŸ”„ ä¸€è‡´æ€§é—®é¢˜è¾ƒå¤šï¼Œå»ºè®®ç»Ÿä¸€æ•°æ®æ ¼å¼å’Œæ¸…ç†é‡å¤è®°å½•")
        
        if metrics.validity.score < 0.8:
            recommendations.append("âœ”ï¸ æœ‰æ•ˆæ€§æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œå»ºè®®å®Œå–„ä¸šåŠ¡è§„åˆ™éªŒè¯")
        
        # 3. åŸºäºé—®é¢˜ç±»å‹çš„å»ºè®®
        issue_types = {}
        for issue in issues:
            issue_types[issue.issue_type] = issue_types.get(issue.issue_type, 0) + 1
        
        if issue_types.get('missing_required_data', 0) > 0:
            recommendations.append("ğŸ“ å‘ç°å¿…å¡«å­—æ®µç¼ºå¤±ï¼Œå»ºè®®åœ¨æ•°æ®å½•å…¥ç•Œé¢å¢åŠ å¿…å¡«éªŒè¯")
        
        if issue_types.get('value_out_of_range', 0) > 0:
            recommendations.append("ğŸ“ å‘ç°æ•°å€¼è¶…å‡ºåˆç†èŒƒå›´ï¼Œå»ºè®®è®¾ç½®æ•°æ®å½•å…¥çš„èŒƒå›´é™åˆ¶")
        
        if issue_types.get('duplicate_records', 0) > 0:
            recommendations.append("ğŸ” å‘ç°é‡å¤è®°å½•ï¼Œå»ºè®®å®æ–½å»é‡ç­–ç•¥å’Œå”¯ä¸€æ€§çº¦æŸ")
        
        # 4. åŸºäºå¼‚å¸¸æ£€æµ‹çš„å»ºè®®
        if anomalies:
            ml_anomalies = [a for a in anomalies if a.detection_method == 'isolation_forest']
            if ml_anomalies:
                recommendations.append("ğŸ¤– æœºå™¨å­¦ä¹ æ£€æµ‹åˆ°å¤æ‚å¼‚å¸¸æ¨¡å¼ï¼Œå»ºè®®æ·±å…¥åˆ†æå¤šç»´åº¦æ•°æ®å…³ç³»")
            
            stat_anomalies = [a for a in anomalies if a.detection_method == 'z_score']
            if stat_anomalies:
                recommendations.append("ğŸ“ˆ ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å‘ç°ç¦»ç¾¤å€¼ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®å½•å…¥çš„å‡†ç¡®æ€§")
        
        # 5. åŸºäºè¶‹åŠ¿çš„å»ºè®®
        if metrics.score_trend == "declining":
            recommendations.append("ğŸ“‰ æ•°æ®è´¨é‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥åŸå› å¹¶é‡‡å–çº æ­£æªæ–½")
        elif metrics.score_trend == "improving":
            recommendations.append("ğŸ“ˆ æ•°æ®è´¨é‡æŒç»­æ”¹å–„ï¼Œå»ºè®®ç»§ç»­å½“å‰çš„è´¨é‡ç®¡ç†ç­–ç•¥")
        
        # 6. è‡ªåŠ¨ä¿®å¤å»ºè®®
        auto_fixable_count = len([i for i in issues if i.auto_fixable])
        if auto_fixable_count > 0:
            recommendations.append(f"ğŸ”§ å‘ç° {auto_fixable_count} ä¸ªå¯è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜ï¼Œå»ºè®®å¯ç”¨è‡ªåŠ¨ä¿®å¤åŠŸèƒ½")
        
        return recommendations[:10]  # é™åˆ¶å»ºè®®æ•°é‡
    
    def _determine_alert_level(self, metrics: EnhancedQualityMetrics, issues: List[EnhancedQualityIssue]) -> str:
        """ç¡®å®šå‘Šè­¦çº§åˆ«"""
        critical_issues = len([i for i in issues if i.severity == 'critical'])
        high_issues = len([i for i in issues if i.severity == 'high'])
        
        if critical_issues > 0 or metrics.quality_grade in ['F', 'D']:
            return "critical"
        elif high_issues > 5 or metrics.quality_grade in ['C', 'C+']:
            return "warning"
        else:
            return "normal"
    
    def _generate_quality_summary(self, metrics: EnhancedQualityMetrics, issues: List[EnhancedQualityIssue]) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æ‘˜è¦"""
        return {
            'overall_grade': metrics.quality_grade,
            'overall_score': round(metrics.weighted_score, 3),
            'dimension_scores': {
                'completeness': round(metrics.completeness.score, 3),
                'accuracy': round(metrics.accuracy.score, 3),
                'consistency': round(metrics.consistency.score, 3),
                'validity': round(metrics.validity.score, 3)
            },
            'issue_summary': {
                'total': len(issues),
                'critical': len([i for i in issues if i.severity == 'critical']),
                'high': len([i for i in issues if i.severity == 'high']),
                'medium': len([i for i in issues if i.severity == 'medium']),
                'low': len([i for i in issues if i.severity == 'low'])
            },
            'trend': metrics.score_trend,
            'auto_fixable': len([i for i in issues if i.auto_fixable])
        }
    
    def _store_quality_history(self, report: EnhancedQualityReport):
        """å­˜å‚¨è´¨é‡å†å²æ•°æ®"""
        self.quality_history.append(report)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€å¤šä¿ç•™100æ¡ï¼‰
        if len(self.quality_history) > 100:
            self.quality_history = self.quality_history[-100:]
        
        # å¯é€‰ï¼šæŒä¹…åŒ–åˆ°æ–‡ä»¶
        try:
            history_file = 'quality_history.json'
            history_data = []
            
            for report in self.quality_history[-10:]:  # åªä¿å­˜æœ€è¿‘10æ¡åˆ°æ–‡ä»¶
                history_data.append({
                    'timestamp': report.timestamp,
                    'overall_score': report.metrics.weighted_score,
                    'quality_grade': report.metrics.quality_grade,
                    'issue_count': report.metrics.issue_count,
                    'critical_issues': report.metrics.critical_issue_count
                })
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"ä¿å­˜è´¨é‡å†å²å¤±è´¥: {e}")
    
    def export_report(self, report: EnhancedQualityReport, format_type: str = 'json') -> str:
        """å¯¼å‡ºå¢å¼ºå‹è´¨é‡æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format_type.lower() == 'json':
            filename = f'enhanced_quality_report_{timestamp}.json'
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            report_dict = asdict(report)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"å¢å¼ºå‹è´¨é‡æŠ¥å‘Šå·²å¯¼å‡º: {filename}")
            return filename
            
        elif format_type.lower() == 'html':
            filename = f'enhanced_quality_report_{timestamp}.html'
            html_content = self._generate_html_report(report)
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            self.logger.info(f"å¢å¼ºå‹HTMLè´¨é‡æŠ¥å‘Šå·²å¯¼å‡º: {filename}")
            return filename
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
    
    def _generate_html_report(self, report: EnhancedQualityReport) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        html_template = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¢å¼ºå‹æ•°æ®è´¨é‡æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #e0e0e0; }}
        .grade {{ font-size: 48px; font-weight: bold; margin: 10px 0; }}
        .grade.A {{ color: #4CAF50; }} .grade.B {{ color: #8BC34A; }} .grade.C {{ color: #FFC107; }} .grade.D {{ color: #FF9800; }} .grade.F {{ color: #F44336; }}
        .metrics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 30px 0; }}
        .metric-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #007bff; }}
        .metric-title {{ font-weight: bold; color: #333; margin-bottom: 10px; }}
        .metric-score {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .issues-section {{ margin: 30px 0; }}
        .issue-item {{ background: #fff; border: 1px solid #ddd; border-radius: 5px; padding: 15px; margin: 10px 0; }}
        .issue-critical {{ border-left: 4px solid #F44336; }} .issue-high {{ border-left: 4px solid #FF9800; }}
        .issue-medium {{ border-left: 4px solid #FFC107; }} .issue-low {{ border-left: 4px solid #4CAF50; }}
        .recommendations {{ background: #e3f2fd; padding: 20px; border-radius: 8px; margin: 20px 0; }}
        .alert-critical {{ background: #ffebee; border: 1px solid #f44336; }}
        .alert-warning {{ background: #fff3e0; border: 1px solid #ff9800; }}
        .alert-normal {{ background: #e8f5e8; border: 1px solid #4caf50; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ” å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§æŠ¥å‘Š</h1>
            <div class="grade {grade_class}">{quality_grade}</div>
            <p>æ€»ä½“è¯„åˆ†: <strong>{overall_score:.1%}</strong></p>
            <p>ç”Ÿæˆæ—¶é—´: {timestamp}</p>
            <p>å‘Šè­¦çº§åˆ«: <span class="alert-{alert_level}">{alert_level_text}</span></p>
        </div>
        
        <div class="metrics">
            <div class="metric-card">
                <div class="metric-title">ğŸ“Š å®Œæ•´æ€§</div>
                <div class="metric-score">{completeness_score:.1%}</div>
                <small>æƒé‡: {completeness_weight:.0%}</small>
            </div>
            <div class="metric-card">
                <div class="metric-title">ğŸ¯ å‡†ç¡®æ€§</div>
                <div class="metric-score">{accuracy_score:.1%}</div>
                <small>æƒé‡: {accuracy_weight:.0%}</small>
            </div>
            <div class="metric-card">
                <div class="metric-title">ğŸ”„ ä¸€è‡´æ€§</div>
                <div class="metric-score">{consistency_score:.1%}</div>
                <small>æƒé‡: {consistency_weight:.0%}</small>
            </div>
            <div class="metric-card">
                <div class="metric-title">âœ”ï¸ æœ‰æ•ˆæ€§</div>
                <div class="metric-score">{validity_score:.1%}</div>
                <small>æƒé‡: {validity_weight:.0%}</small>
            </div>
        </div>
        
        <div class="issues-section">
            <h2>ğŸ“‹ è´¨é‡é—®é¢˜è¯¦æƒ… ({issue_count} ä¸ªé—®é¢˜)</h2>
            {issues_html}
        </div>
        
        <div class="recommendations">
            <h2>ğŸ’¡ æ™ºèƒ½å»ºè®®</h2>
            {recommendations_html}
        </div>
        
        <div style="margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd; text-align: center; color: #666;">
            <small>æŠ¥å‘ŠID: {report_id} | å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’</small>
        </div>
    </div>
</body>
</html>
        """
        
        # å‡†å¤‡æ•°æ®
        grade_class = report.metrics.quality_grade[0] if report.metrics.quality_grade else 'F'
        alert_level_map = {'critical': 'ä¸¥é‡', 'warning': 'è­¦å‘Š', 'normal': 'æ­£å¸¸'}
        
        # ç”Ÿæˆé—®é¢˜HTML
        issues_html = ""
        for issue in report.issues[:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            severity_class = f"issue-{issue.severity}"
            issues_html += f"""
            <div class="issue-item {severity_class}">
                <strong>{issue.issue_type}</strong> - {issue.severity.upper()}
                <p>{issue.description}</p>
                <small>è¡¨: {issue.table_name} | åˆ—: {issue.column_name} | å½±å“è®°å½•: {issue.affected_records}</small>
                <p><em>å»ºè®®: {issue.suggested_action}</em></p>
            </div>
            """
        
        # ç”Ÿæˆå»ºè®®HTML
        recommendations_html = "<ul>"
        for rec in report.recommendations:
            recommendations_html += f"<li>{rec}</li>"
        recommendations_html += "</ul>"
        
        return html_template.format(
            grade_class=grade_class,
            quality_grade=report.metrics.quality_grade,
            overall_score=report.metrics.weighted_score,
            timestamp=report.timestamp,
            alert_level=report.alert_level,
            alert_level_text=alert_level_map.get(report.alert_level, 'æœªçŸ¥'),
            completeness_score=report.metrics.completeness.score,
            completeness_weight=report.metrics.completeness.weight,
            accuracy_score=report.metrics.accuracy.score,
            accuracy_weight=report.metrics.accuracy.weight,
            consistency_score=report.metrics.consistency.score,
            consistency_weight=report.metrics.consistency.weight,
            validity_score=report.metrics.validity.score,
            validity_weight=report.metrics.validity.weight,
            issue_count=len(report.issues),
            issues_html=issues_html,
            recommendations_html=recommendations_html,
            report_id=report.report_id,
            processing_time=report.processing_time
        )
    
    def print_summary(self, report: EnhancedQualityReport):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ” å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§æŠ¥å‘Šæ‘˜è¦")
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {report.timestamp}")
        print(f"ğŸ†” æŠ¥å‘ŠID: {report.report_id}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {report.processing_time:.2f} ç§’")
        print(f"ğŸš¨ å‘Šè­¦çº§åˆ«: {report.alert_level.upper()}")
        
        # è´¨é‡è¯„åˆ†
        print(f"\nğŸ“Š æ•´ä½“è´¨é‡è¯„åˆ†")
        print(f"   ç­‰çº§: {report.metrics.quality_grade} ({report.metrics.weighted_score:.1%})")
        print(f"   è¶‹åŠ¿: {report.metrics.score_trend}")
        
        # ç»´åº¦åˆ†æ•°
        print(f"\nğŸ“ˆ å„ç»´åº¦è¯„åˆ†:")
        print(f"   ğŸ“Š å®Œæ•´æ€§: {report.metrics.completeness.score:.1%} (æƒé‡: {report.metrics.completeness.weight:.0%})")
        print(f"   ğŸ¯ å‡†ç¡®æ€§: {report.metrics.accuracy.score:.1%} (æƒé‡: {report.metrics.accuracy.weight:.0%})")
        print(f"   ğŸ”„ ä¸€è‡´æ€§: {report.metrics.consistency.score:.1%} (æƒé‡: {report.metrics.consistency.weight:.0%})")
        print(f"   âœ”ï¸ æœ‰æ•ˆæ€§: {report.metrics.validity.score:.1%} (æƒé‡: {report.metrics.validity.weight:.0%})")
        
        # é—®é¢˜ç»Ÿè®¡
        print(f"\nğŸ“‹ é—®é¢˜ç»Ÿè®¡:")
        print(f"   æ€»é—®é¢˜æ•°: {len(report.issues)}")
        print(f"   ä¸¥é‡é—®é¢˜: {len([i for i in report.issues if i.severity == 'critical'])}")
        print(f"   é«˜çº§é—®é¢˜: {len([i for i in report.issues if i.severity == 'high'])}")
        print(f"   ä¸­çº§é—®é¢˜: {len([i for i in report.issues if i.severity == 'medium'])}")
        print(f"   ä½çº§é—®é¢˜: {len([i for i in report.issues if i.severity == 'low'])}")
        print(f"   å¯è‡ªåŠ¨ä¿®å¤: {len([i for i in report.issues if i.auto_fixable])}")
        
        # å¼‚å¸¸æ£€æµ‹
        if report.anomalies:
            print(f"\nğŸ¤– å¼‚å¸¸æ£€æµ‹:")
            print(f"   æ£€æµ‹åˆ°å¼‚å¸¸: {len(report.anomalies)} ä¸ª")
            for anomaly in report.anomalies[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"   - {anomaly.anomaly_type} ({anomaly.detection_method}): {len(anomaly.affected_records)} æ¡è®°å½•")
        
        # æ™ºèƒ½å»ºè®®
        print(f"\nğŸ’¡ æ™ºèƒ½å»ºè®® (å‰5æ¡):")
        for i, rec in enumerate(report.recommendations[:5], 1):
            print(f"   {i}. {rec}")
        
        print("="*80)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ...")
    
    try:
        # åˆå§‹åŒ–ç›‘æ§å™¨
        monitor = EnhancedDataQualityMonitor()
        
        # è¿è¡Œè´¨é‡æ£€æŸ¥
        report = monitor.run_enhanced_quality_check()
        
        # æ‰“å°æ‘˜è¦
        monitor.print_summary(report)
        
        # å¯¼å‡ºæŠ¥å‘Š
        json_file = monitor.export_report(report, 'json')
        html_file = monitor.export_report(report, 'html')
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²å¯¼å‡º:")
        print(f"   JSON: {json_file}")
        print(f"   HTML: {html_file}")
        
        # æ ¹æ®å‘Šè­¦çº§åˆ«ç»™å‡ºå»ºè®®
        if report.alert_level == 'critical':
            print(f"\nğŸš¨ ä¸¥é‡å‘Šè­¦: æ•°æ®è´¨é‡é—®é¢˜ä¸¥é‡ï¼Œå»ºè®®ç«‹å³å¤„ç†ï¼")
        elif report.alert_level == 'warning':
            print(f"\nâš ï¸ è­¦å‘Š: å‘ç°æ•°æ®è´¨é‡é—®é¢˜ï¼Œå»ºè®®åŠæ—¶å¤„ç†ã€‚")
        else:
            print(f"\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        return report
        
    except Exception as e:
        logger.error(f"å¢å¼ºå‹æ•°æ®è´¨é‡ç›‘æ§å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()