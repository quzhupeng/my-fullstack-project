#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•å·¥å…·
ç‰ˆæœ¬: 1.0
ä½œè€…: Kilo Code
æ—¥æœŸ: 2025-01-05

åŠŸèƒ½:
1. æ€§èƒ½åŸºå‡†æµ‹è¯•
2. å†…å­˜ä½¿ç”¨ç›‘æ§
3. å¤„ç†æ—¶é—´åˆ†æ
4. ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
"""

import pandas as pd
import numpy as np
import time
import psutil
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass
import json
import os
import gc
from functools import wraps
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ä¼˜åŒ–çš„æ¨¡å—
try:
    from optimized_data_importer import OptimizedDataImporter
    from optimized_production_sales_ratio import ProductionSalesRatioAnalyzer
    from data_quality_monitor import DataQualityMonitor
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ä¼˜åŒ–æ¨¡å— - {e}")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('performance_optimizer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    operation_name: str
    execution_time: float
    memory_before: float
    memory_after: float
    memory_peak: float
    cpu_usage: float
    records_processed: int
    throughput: float  # è®°å½•/ç§’
    timestamp: str

@dataclass
class OptimizationReport:
    """ä¼˜åŒ–æŠ¥å‘Šæ•°æ®ç±»"""
    report_id: str
    timestamp: str
    system_info: Dict[str, Any]
    performance_metrics: List[PerformanceMetrics]
    bottlenecks: List[str]
    recommendations: List[str]
    overall_score: float

def performance_monitor(func: Callable) -> Callable:
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        cpu_before = process.cpu_percent()
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        cpu_after = process.cpu_percent()
        
        # è®¡ç®—æŒ‡æ ‡
        execution_time = end_time - start_time
        memory_peak = max(memory_before, memory_after)
        cpu_usage = (cpu_before + cpu_after) / 2
        
        # è®°å½•æ€§èƒ½æ•°æ®
        metrics = PerformanceMetrics(
            operation_name=func.__name__,
            execution_time=execution_time,
            memory_before=memory_before,
            memory_after=memory_after,
            memory_peak=memory_peak,
            cpu_usage=cpu_usage,
            records_processed=0,  # éœ€è¦åœ¨å‡½æ•°ä¸­è®¾ç½®
            throughput=0,
            timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"{func.__name__} æ‰§è¡Œå®Œæˆ - è€—æ—¶: {execution_time:.2f}s, å†…å­˜: {memory_peak:.1f}MB")
        
        return result, metrics
    
    return wrapper

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, excel_folder: str = './Excelæ–‡ä»¶å¤¹/'):
        self.excel_folder = excel_folder
        self.logger = logging.getLogger(f"{__name__}.PerformanceOptimizer")
        self.metrics_history = []
        
        # æ€§èƒ½é˜ˆå€¼é…ç½®
        self.performance_thresholds = {
            'execution_time': {
                'excellent': 5.0,    # ç§’
                'good': 15.0,
                'acceptable': 30.0,
                'poor': 60.0
            },
            'memory_usage': {
                'excellent': 100,    # MB
                'good': 250,
                'acceptable': 500,
                'poor': 1000
            },
            'throughput': {
                'excellent': 1000,   # è®°å½•/ç§’
                'good': 500,
                'acceptable': 100,
                'poor': 50
            }
        }
    
    def run_performance_test(self) -> OptimizationReport:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
        self.logger.info("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        start_time = datetime.now()
        
        try:
            # æ”¶é›†ç³»ç»Ÿä¿¡æ¯
            system_info = self._collect_system_info()
            
            # è¿è¡Œå„é¡¹æ€§èƒ½æµ‹è¯•
            all_metrics = []
            
            # 1. æ•°æ®å¯¼å…¥æ€§èƒ½æµ‹è¯•
            import_metrics = self._test_data_import_performance()
            all_metrics.extend(import_metrics)
            
            # 2. äº§é”€ç‡è®¡ç®—æ€§èƒ½æµ‹è¯•
            ratio_metrics = self._test_ratio_calculation_performance()
            all_metrics.extend(ratio_metrics)
            
            # 3. æ•°æ®è´¨é‡æ£€æŸ¥æ€§èƒ½æµ‹è¯•
            quality_metrics = self._test_quality_check_performance()
            all_metrics.extend(quality_metrics)
            
            # 4. å†…å­˜ä¼˜åŒ–æµ‹è¯•
            memory_metrics = self._test_memory_optimization()
            all_metrics.extend(memory_metrics)
            
            # åˆ†æç“¶é¢ˆ
            bottlenecks = self._identify_bottlenecks(all_metrics)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = self._generate_optimization_recommendations(all_metrics, bottlenecks)
            
            # è®¡ç®—æ•´ä½“æ€§èƒ½åˆ†æ•°
            overall_score = self._calculate_overall_score(all_metrics)
            
            # åˆ›å»ºæŠ¥å‘Š
            report = OptimizationReport(
                report_id=f"PERF_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                timestamp=start_time.isoformat(),
                system_info=system_info,
                performance_metrics=all_metrics,
                bottlenecks=bottlenecks,
                recommendations=recommendations,
                overall_score=overall_score
            )
            
            self.logger.info("æ€§èƒ½æµ‹è¯•å®Œæˆ")
            return report
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def _collect_system_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        return {
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
            'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
            'memory_available': psutil.virtual_memory().available / 1024 / 1024 / 1024,  # GB
            'disk_usage': psutil.disk_usage('.').percent,
            'python_version': f"{psutil.sys.version_info.major}.{psutil.sys.version_info.minor}",
            'pandas_version': pd.__version__,
            'numpy_version': np.__version__
        }
    
    def _test_data_import_performance(self) -> List[PerformanceMetrics]:
        """æµ‹è¯•æ•°æ®å¯¼å…¥æ€§èƒ½"""
        self.logger.info("æµ‹è¯•æ•°æ®å¯¼å…¥æ€§èƒ½...")
        metrics = []
        
        try:
            # æµ‹è¯•åŸå§‹å¯¼å…¥æ–¹æ³•
            original_metrics = self._benchmark_original_import()
            if original_metrics:
                metrics.append(original_metrics)
            
            # æµ‹è¯•ä¼˜åŒ–å¯¼å…¥æ–¹æ³•
            optimized_metrics = self._benchmark_optimized_import()
            if optimized_metrics:
                metrics.append(optimized_metrics)
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å¯¼å…¥æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return metrics
    
    def _benchmark_original_import(self) -> Optional[PerformanceMetrics]:
        """åŸºå‡†æµ‹è¯•åŸå§‹å¯¼å…¥æ–¹æ³•"""
        try:
            start_time = time.time()
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # æ¨¡æ‹ŸåŸå§‹å¯¼å…¥é€»è¾‘
            total_records = 0
            for filename in ['é”€å”®å‘ç¥¨æ‰§è¡ŒæŸ¥è¯¢.xlsx', 'æ”¶å‘å­˜æ±‡æ€»è¡¨æŸ¥è¯¢.xlsx']:
                filepath = os.path.join(self.excel_folder, filename)
                if os.path.exists(filepath):
                    df = pd.read_excel(filepath)
                    total_records += len(df)
                    # ç®€å•çš„æ•°æ®å¤„ç†
                    df = df.dropna()
                    df = df[df.select_dtypes(include=[np.number]).columns[0] > 0] if len(df.select_dtypes(include=[np.number]).columns) > 0 else df
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            return PerformanceMetrics(
                operation_name="original_data_import",
                execution_time=end_time - start_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=max(memory_before, memory_after),
                cpu_usage=process.cpu_percent(),
                records_processed=total_records,
                throughput=total_records / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            self.logger.error(f"åŸå§‹å¯¼å…¥åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _benchmark_optimized_import(self) -> Optional[PerformanceMetrics]:
        """åŸºå‡†æµ‹è¯•ä¼˜åŒ–å¯¼å…¥æ–¹æ³•"""
        try:
            start_time = time.time()
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # ä½¿ç”¨ä¼˜åŒ–çš„å¯¼å…¥å™¨
            importer = OptimizedDataImporter(self.excel_folder)
            result = importer.process_all_data()
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            total_records = sum(len(df) for df in result.processed_data.values() if df is not None)
            
            return PerformanceMetrics(
                operation_name="optimized_data_import",
                execution_time=end_time - start_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=max(memory_before, memory_after),
                cpu_usage=process.cpu_percent(),
                records_processed=total_records,
                throughput=total_records / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–å¯¼å…¥åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _test_ratio_calculation_performance(self) -> List[PerformanceMetrics]:
        """æµ‹è¯•äº§é”€ç‡è®¡ç®—æ€§èƒ½"""
        self.logger.info("æµ‹è¯•äº§é”€ç‡è®¡ç®—æ€§èƒ½...")
        metrics = []
        
        try:
            start_time = time.time()
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # ä½¿ç”¨ä¼˜åŒ–çš„äº§é”€ç‡åˆ†æå™¨
            analyzer = ProductionSalesRatioAnalyzer(self.excel_folder)
            sales_data, inventory_data = analyzer.load_and_validate_data()
            report = analyzer.calculate_production_sales_ratio(sales_data, inventory_data)
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            total_records = len(sales_data) + len(inventory_data)
            
            metrics.append(PerformanceMetrics(
                operation_name="ratio_calculation",
                execution_time=end_time - start_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=max(memory_before, memory_after),
                cpu_usage=process.cpu_percent(),
                records_processed=total_records,
                throughput=total_records / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                timestamp=datetime.now().isoformat()
            ))
            
        except Exception as e:
            self.logger.error(f"äº§é”€ç‡è®¡ç®—æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return metrics
    
    def _test_quality_check_performance(self) -> List[PerformanceMetrics]:
        """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥æ€§èƒ½"""
        self.logger.info("æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥æ€§èƒ½...")
        metrics = []
        
        try:
            start_time = time.time()
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # ä½¿ç”¨æ•°æ®è´¨é‡ç›‘æ§å™¨
            monitor = DataQualityMonitor(self.excel_folder)
            report = monitor.run_quality_check()
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            metrics.append(PerformanceMetrics(
                operation_name="quality_check",
                execution_time=end_time - start_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=max(memory_before, memory_after),
                cpu_usage=process.cpu_percent(),
                records_processed=report.metrics.total_records,
                throughput=report.metrics.total_records / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                timestamp=datetime.now().isoformat()
            ))
            
        except Exception as e:
            self.logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return metrics
    
    def _test_memory_optimization(self) -> List[PerformanceMetrics]:
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        self.logger.info("æµ‹è¯•å†…å­˜ä¼˜åŒ–...")
        metrics = []
        
        try:
            # æµ‹è¯•å¤§æ•°æ®é›†å¤„ç†
            start_time = time.time()
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # åˆ›å»ºå¤§æ•°æ®é›†è¿›è¡Œæµ‹è¯•
            large_df = pd.DataFrame({
                'col1': np.random.randn(100000),
                'col2': np.random.randn(100000),
                'col3': np.random.choice(['A', 'B', 'C'], 100000),
                'col4': pd.date_range('2023-01-01', periods=100000, freq='H')
            })
            
            # æ‰§è¡Œä¸€äº›å†…å­˜å¯†é›†å‹æ“ä½œ
            result = large_df.groupby('col3').agg({
                'col1': ['mean', 'std', 'min', 'max'],
                'col2': ['sum', 'count']
            })
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            del large_df
            gc.collect()
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            metrics.append(PerformanceMetrics(
                operation_name="memory_optimization_test",
                execution_time=end_time - start_time,
                memory_before=memory_before,
                memory_after=memory_after,
                memory_peak=max(memory_before, memory_after),
                cpu_usage=process.cpu_percent(),
                records_processed=100000,
                throughput=100000 / (end_time - start_time) if (end_time - start_time) > 0 else 0,
                timestamp=datetime.now().isoformat()
            ))
            
        except Exception as e:
            self.logger.error(f"å†…å­˜ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        
        return metrics
    
    def _identify_bottlenecks(self, metrics: List[PerformanceMetrics]) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        for metric in metrics:
            # æ£€æŸ¥æ‰§è¡Œæ—¶é—´ç“¶é¢ˆ
            if metric.execution_time > self.performance_thresholds['execution_time']['poor']:
                bottlenecks.append(f"{metric.operation_name}: æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({metric.execution_time:.2f}s)")
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç“¶é¢ˆ
            if metric.memory_peak > self.performance_thresholds['memory_usage']['poor']:
                bottlenecks.append(f"{metric.operation_name}: å†…å­˜ä½¿ç”¨è¿‡é«˜ ({metric.memory_peak:.1f}MB)")
            
            # æ£€æŸ¥ååé‡ç“¶é¢ˆ
            if metric.throughput < self.performance_thresholds['throughput']['poor']:
                bottlenecks.append(f"{metric.operation_name}: å¤„ç†é€Ÿåº¦è¿‡æ…¢ ({metric.throughput:.1f} è®°å½•/ç§’)")
        
        return bottlenecks
    
    def _generate_optimization_recommendations(self, metrics: List[PerformanceMetrics], 
                                             bottlenecks: List[str]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        if any("æ‰§è¡Œæ—¶é—´è¿‡é•¿" in b for b in bottlenecks):
            recommendations.append("è€ƒè™‘ä½¿ç”¨å¹¶è¡Œå¤„ç†æˆ–åˆ†å—å¤„ç†æ¥å‡å°‘æ‰§è¡Œæ—¶é—´")
            recommendations.append("ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—")
        
        if any("å†…å­˜ä½¿ç”¨è¿‡é«˜" in b for b in bottlenecks):
            recommendations.append("ä½¿ç”¨åˆ†å—è¯»å–å¤§æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®")
            recommendations.append("åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å˜é‡ï¼Œä½¿ç”¨åƒåœ¾å›æ”¶")
            recommendations.append("è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç±»å‹ï¼ˆå¦‚categoryç±»å‹ï¼‰")
        
        if any("å¤„ç†é€Ÿåº¦è¿‡æ…¢" in b for b in bottlenecks):
            recommendations.append("ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯")
            recommendations.append("ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œä½¿ç”¨æ‰¹é‡æ“ä½œ")
            recommendations.append("è€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ–‡ä»¶æ ¼å¼ï¼ˆå¦‚Parquetï¼‰")
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        avg_memory = np.mean([m.memory_peak for m in metrics])
        if avg_memory > 200:
            recommendations.append("æ•´ä½“å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç»“æ„")
        
        avg_time = np.mean([m.execution_time for m in metrics])
        if avg_time > 10:
            recommendations.append("æ•´ä½“å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®è¿›è¡Œæ€§èƒ½è°ƒä¼˜")
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾é—®é¢˜
        if not bottlenecks:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥çš„å¾®è°ƒä¼˜åŒ–")
        
        return recommendations
    
    def _calculate_overall_score(self, metrics: List[PerformanceMetrics]) -> float:
        """è®¡ç®—æ•´ä½“æ€§èƒ½åˆ†æ•°"""
        if not metrics:
            return 0.0
        
        scores = []
        
        for metric in metrics:
            # æ—¶é—´åˆ†æ•° (è¶ŠçŸ­è¶Šå¥½)
            time_score = 1.0
            if metric.execution_time > self.performance_thresholds['execution_time']['excellent']:
                time_score = max(0, 1 - (metric.execution_time - self.performance_thresholds['execution_time']['excellent']) / 60)
            
            # å†…å­˜åˆ†æ•° (è¶Šå°‘è¶Šå¥½)
            memory_score = 1.0
            if metric.memory_peak > self.performance_thresholds['memory_usage']['excellent']:
                memory_score = max(0, 1 - (metric.memory_peak - self.performance_thresholds['memory_usage']['excellent']) / 1000)
            
            # ååé‡åˆ†æ•° (è¶Šé«˜è¶Šå¥½)
            throughput_score = min(1.0, metric.throughput / self.performance_thresholds['throughput']['excellent'])
            
            # ç»¼åˆåˆ†æ•°
            overall_metric_score = (time_score + memory_score + throughput_score) / 3
            scores.append(overall_metric_score)
        
        return round(np.mean(scores), 3)
    
    def export_report(self, report: OptimizationReport, format: str = 'json') -> str:
        """å¯¼å‡ºä¼˜åŒ–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format.lower() == 'json':
            filename = f"performance_report_{timestamp}.json"
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            report_dict = {
                'report_id': report.report_id,
                'timestamp': report.timestamp,
                'system_info': report.system_info,
                'performance_metrics': [
                    {
                        'operation_name': m.operation_name,
                        'execution_time': m.execution_time,
                        'memory_before': m.memory_before,
                        'memory_after': m.memory_after,
                        'memory_peak': m.memory_peak,
                        'cpu_usage': m.cpu_usage,
                        'records_processed': m.records_processed,
                        'throughput': m.throughput,
                        'timestamp': m.timestamp
                    } for m in report.performance_metrics
                ],
                'bottlenecks': report.bottlenecks,
                'recommendations': report.recommendations,
                'overall_score': report.overall_score
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²å¯¼å‡º: {filename}")
        return filename
    
    def print_summary(self, report: OptimizationReport):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šæ‘˜è¦")
        print("=" * 80)
        print(f"æŠ¥å‘ŠID: {report.report_id}")
        print(f"ç”Ÿæˆæ—¶é—´: {report.timestamp}")
        print(f"æ•´ä½“æ€§èƒ½åˆ†æ•°: {report.overall_score:.1%}")
        
        print(f"\nç³»ç»Ÿä¿¡æ¯:")
        print(f"  CPUæ ¸å¿ƒæ•°: {report.system_info['cpu_count']}")
        print(f"  æ€»å†…å­˜: {report.system_info['memory_total']:.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {report.system_info['memory_available']:.1f} GB")
        print(f"  Pythonç‰ˆæœ¬: {report.system_info['python_version']}")
        
        print(f"\næ€§èƒ½æŒ‡æ ‡:")
        for metric in report.performance_metrics:
            print(f"  {metric.operation_name}:")
            print(f"    æ‰§è¡Œæ—¶é—´: {metric.execution_time:.2f}s")
            print(f"    å†…å­˜å³°å€¼: {metric.memory_peak:.1f}MB")
            print(f"    å¤„ç†è®°å½•: {metric.records_processed:,}")
            print(f"    ååé‡: {metric.throughput:.1f} è®°å½•/ç§’")
        
        if report.bottlenecks:
            print(f"\næ€§èƒ½ç“¶é¢ˆ:")
            for bottleneck in report.bottlenecks:
                print(f"  âš ï¸ {bottleneck}")
        
        print(f"\nä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(report.recommendations, 1):
            print(f"  {i}. {rec}")
        
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•å·¥å…· v1.0")
    print("=" * 80)
    
    try:
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = PerformanceOptimizer()
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        report = optimizer.run_performance_test()
        
        # æ‰“å°æ‘˜è¦
        optimizer.print_summary(report)
        
        # å¯¼å‡ºæŠ¥å‘Š
        json_file = optimizer.export_report(report)
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {json_file}")
        
    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    main()